{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 03: Using Finetuned Prithvi model for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Inferencing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [NVIDIA](https://developer.nvidia.com/topics/ai/ai-inference), Inference (Machine Learning / AI) *\"is the process of generating outputs from a model by providing it inputs. There are numerous types of data inputs and outputs—such as images, text, or video—that are used to produce applications such as a weather forecast or a conversation with a large language model (LLM).\"*\n",
    "\n",
    "For our specific use case, Inferencing is the part where we use the finetuned Prithvi model to predict Land Use classes based on the 6 classes we used for training. To recap, the 6 classes that we have are:\n",
    "\n",
    "| Land Use | Raster Value | Color Code |\n",
    "|----------|----|----|\n",
    "| Urban | 1 | red |\n",
    "| Agricultural | 2 | yellow |\n",
    "| Forest | 3 | green |\n",
    "| Water | 4 | blue |\n",
    "| Oil Palm | 5 | purple |\n",
    "| Para Rubber | 6 | pink |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Inference scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before running the inferencing, you need to have:\n",
    "\n",
    "1. Pre-trained or Finetuned Prithvi model\n",
    "2. Target dataset (HLS / Landsat / Sentinel) with 18 bands (6 bands x 3 time periods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Target datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we prepared two sample images:\n",
    "1. Bangkok\n",
    "2. Ubon Ratchathani\n",
    "\n",
    "NOTE: These two images are mosaics of multiple HLS images and might contain some blank regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://rgw.glodal-inc.net/public/03_inference.cropped.tar.xz\n",
    "!tar xaf 03_inference.cropped.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Visualizing Bangkok and Ubon-Ratchathani\n",
    "# ESTIMATED RUNTIME: <1 minute, if the cell runs for more than 1min, restart the kernel\n",
    "\n",
    "import os\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Visualizing the sample rasters\n",
    "cm_path = \"target_datasets_cropped/chiang-mai_18bands.tif\"\n",
    "kk_path = \"target_datasets_cropped/khon-kaen_18bands.tif\"\n",
    "st_path = \"target_datasets_cropped/sura-thani_18bands.tif\"\n",
    "\n",
    "def visualize_raster(raster_path,name):\n",
    "    with rio.open(raster_path) as src:\n",
    "        data = src.read()\n",
    "        print(\"\\nImage shape:\",data.shape) #check if the target images has 18bands\n",
    "\n",
    "        # visualize the HLS file\n",
    "        rgb = src.read([3,2,1]) # change to [3,2,1] for RGB. [4,3,2] shows False Color\n",
    "\n",
    "        # scale to 0-255 for visualization\n",
    "        scaled_rgb = np.zeros_like(rgb, dtype=np.uint8)\n",
    "\n",
    "        for i in range(3):\n",
    "            band = rgb[i]\n",
    "            min_val = np.min(band[band > 0]) #\n",
    "            max_val = np.max(band[band > 0]) # band > 0 will select pixel values that are positive\n",
    "            scaled_rgb[i] =  ((band - min_val) / (max_val - min_val)*255).astype(np.uint8)\n",
    "\n",
    "        scaled_rgb_new = scaled_rgb.transpose((1,2,0))\n",
    "\n",
    "        # display the RGB image\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(scaled_rgb_new)\n",
    "        plt.title(name)\n",
    "        plt.show()\n",
    "        \n",
    "# bangkok\n",
    "visualize_raster(cm_path,\"Chiang Mai\")\n",
    "visualize_raster(kk_path,\"Khon Kaen\")\n",
    "visualize_raster(st_path,\"Sura Thani\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the actual inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Due to a relatively long code, the succeeding cells should be executed/run in sequence as we run the inference cell by cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ESTIMATED RUNTIME: <1 minute\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import torch\n",
    "from mmcv import Config\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmseg.apis import init_segmentor\n",
    "from mmseg.datasets.pipelines import Compose, LoadImageFromFile\n",
    "\n",
    "## DEFINE PARAMETERS, change the paths as necessary\n",
    "config_path = 'finetuning_config.py'\n",
    "ckpt = 'exp-outputs/exp02/best_mIoU_epoch_42.pth' ###### TO SPECIFY THE MODEL FILE *.pth ######\n",
    "input_path = 'target_datasets_cropped' # path to input images folder for inference\n",
    "input_type = 'tif' #file type of input images\n",
    "output_path = 'output/inference_outputs' #path to save output image\n",
    "bands = None # bands in the file where to find the relevant data, type = int. Use only if there are specific bands in the target dataset where the data is\n",
    "device = 'cuda' #default value is cuda, change accordingly\n",
    "\n",
    "## ACTUAL INFERENCE\n",
    "#### STEP 1: load the model\n",
    "print(\"\\n>>> Step 1: Loading the model <<<\")\n",
    "config = Config.fromfile(config_path)\n",
    "config.model.backbone.pretrained = None\n",
    "model = init_segmentor(config, ckpt, device)\n",
    "\n",
    "#### STEP 2: locate the target images\n",
    "print(\"\\n>>> Step 2: Locate the target images <<<\")\n",
    "target_images = glob.glob(os.path.join(input_path, \"*.\" + input_type))\n",
    "print(\"No of target images:\",len(target_images))\n",
    "\n",
    "# check if output folder available\n",
    "if not os.path.isdir(output_path):\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "#### STEP 3: Test Pipeline\n",
    "#NOTE:THIS IS ONLY FOR IMAGES THAT HAVE SPECIFIC TARGET BANDS\n",
    "print(\"\\n>>> Step 3: Modify the test pipeline if necessary <<<\")\n",
    "\n",
    "def process_test_pipeline(custom_test_pipeline, bands=None):\n",
    "    # change extracted bands if necessary\n",
    "    if bands is not None:\n",
    "        extract_index = [\n",
    "            i for i, x in enumerate(custom_test_pipeline) if x[\"type\"] == \"BandsExtract\"\n",
    "        ]\n",
    "\n",
    "        if len(extract_index) > 0:\n",
    "            custom_test_pipeline[extract_index[0]][\"bands\"] = bands\n",
    "\n",
    "    collect_index = [\n",
    "        i for i, x in enumerate(custom_test_pipeline) if x[\"type\"].find(\"Collect\") > -1\n",
    "    ]\n",
    "\n",
    "    # adapt collected keys if necessary\n",
    "    if len(collect_index) > 0:\n",
    "        keys = [\n",
    "            \"img_info\",\n",
    "            \"filename\",\n",
    "            \"ori_filename\",\n",
    "            \"img\",\n",
    "            \"img_shape\",\n",
    "            \"ori_shape\",\n",
    "            \"pad_shape\",\n",
    "            \"scale_factor\",\n",
    "            \"img_norm_cfg\",\n",
    "        ]\n",
    "        custom_test_pipeline[collect_index[0]][\"meta_keys\"] = keys\n",
    "\n",
    "    return custom_test_pipeline\n",
    "\n",
    "# run the function\n",
    "custom_test_pipeline = process_test_pipeline(model.cfg.data.test.pipeline, bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ESTIMATED RUNTIME: 6 - 15 minutes\n",
    "# Some parts of the code are from the model_inference.py script available from the Prithvi github\n",
    "# IMPORTANT: The larger the image for prediction, the more memory and CPU utilization is expected. Runtime also takes a lot longer\n",
    "\n",
    "#### STEP 4: Iterate prediction for every output image\n",
    "print(\"\\n>>> Step 4: Iterate prediction for every output image <<<\")\n",
    "\n",
    "# define a function to open/read a tif file\n",
    "def open_tiff(fname):\n",
    "    with rasterio.open(fname, \"r\") as src:\n",
    "        data = src.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "# define a function to write a tif file\n",
    "def write_tiff(img_wrt, filename, metadata):\n",
    "    \"\"\"\n",
    "    It writes a raster image to file.\n",
    "\n",
    "    :img_wrt: numpy array containing the data (can be 2D for single band or 3D for multiple bands)\n",
    "    :filename: file path to the output file\n",
    "    :metadata: metadata to use to write the raster to disk\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    with rasterio.open(filename, \"w\", **metadata) as dest:\n",
    "        if len(img_wrt.shape) == 2:\n",
    "            img_wrt = img_wrt[None]\n",
    "\n",
    "        for i in range(img_wrt.shape[0]):\n",
    "            dest.write(img_wrt[i, :, :], i + 1)\n",
    "\n",
    "    return filename\n",
    "\n",
    "# define a function get the metadata of an image\n",
    "def get_meta(fname):\n",
    "    with rasterio.open(fname, \"r\") as src:\n",
    "        meta = src.meta\n",
    "\n",
    "    return meta\n",
    "\n",
    "# define function to perform inference\n",
    "def inference_segmentor(model, imgs, custom_test_pipeline=None):\n",
    "    \"\"\"Inference image(s) with the segmentor.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded segmentor.\n",
    "        imgs (str/ndarray or list[str/ndarray]): Either image files or loaded\n",
    "            images.\n",
    "\n",
    "    Returns:\n",
    "        (list[Tensor]): The segmentation result.\n",
    "    \"\"\"\n",
    "    cfg = model.cfg\n",
    "    device = next(model.parameters()).device  # model device\n",
    "    # build the data pipeline\n",
    "    test_pipeline = (\n",
    "        [LoadImageFromFile()] + cfg.data.test.pipeline[1:]\n",
    "        if custom_test_pipeline == None\n",
    "        else custom_test_pipeline\n",
    "    )\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    # prepare data\n",
    "    data = []\n",
    "    imgs = imgs if isinstance(imgs, list) else [imgs]\n",
    "    for img in imgs:\n",
    "        img_data = {\"img_info\": {\"filename\": img}}\n",
    "        img_data = test_pipeline(img_data)\n",
    "        data.append(img_data)\n",
    "    # print(data.shape)\n",
    "\n",
    "    data = collate(data, samples_per_gpu=len(imgs))\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # data = collate(data, samples_per_gpu=len(imgs))\n",
    "        # scatter to specified GPU\n",
    "        data = scatter(data, [device])[0]\n",
    "    else:\n",
    "        # img_metas = scatter(data['img_metas'],'cpu')\n",
    "        # data['img_metas'] = [i.data[0] for i in data['img_metas']]\n",
    "\n",
    "        img_metas = data[\"img_metas\"].data[0]\n",
    "        img = data[\"img\"]\n",
    "        data = {\"img\": img, \"img_metas\": img_metas}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        result = model(return_loss=False, rescale=True, **data)\n",
    "    return result\n",
    "\n",
    "# define function to perform inference on each target file\n",
    "def inference_on_file(model, target_image, output_image, custom_test_pipeline):\n",
    "    time_taken = -1\n",
    "    try:\n",
    "        st = time.time()\n",
    "        print(\"Running inference...\")\n",
    "        result = inference_segmentor(model, target_image, custom_test_pipeline)\n",
    "        print(\"Output has shape: \" + str(result[0].shape))\n",
    "\n",
    "        # get metadata mask\n",
    "        mask = open_tiff(target_image)\n",
    "        meta = get_meta(target_image)\n",
    "        mask = np.where(mask == meta[\"nodata\"], 1, 0)\n",
    "        mask = np.max(mask, axis=0)[None]\n",
    "\n",
    "        result[0] = np.where(mask == 1, -1, result[0])\n",
    "\n",
    "        # sve file to disk\n",
    "        meta[\"count\"] = 1\n",
    "        meta[\"dtype\"] = \"int16\"\n",
    "        meta[\"compress\"] = \"lzw\"\n",
    "        meta[\"nodata\"] = -1\n",
    "        print(\"Saving output...\")\n",
    "        write_tiff(result[0], output_image, meta)\n",
    "        et = time.time()\n",
    "        time_taken = np.round(et - st, 1)\n",
    "        print(\n",
    "            f\"Inference completed in {str(time_taken)} seconds. Output available at: \"\n",
    "            + output_image\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        print(f\"Error on image {target_image} \\nContinue to next input\")\n",
    "\n",
    "    return time_taken\n",
    "\n",
    "# for each image predict and save to disk\n",
    "for i, target_image in enumerate(target_images):\n",
    "    print(\"\\n> Working on Image {} <\".format(i+1))\n",
    "    output_image = os.path.join(\n",
    "        output_path,\n",
    "        target_image.split(\"/\")[-1].replace(\n",
    "            \".\" + input_type, \"_pred.\" + input_type\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    inference_on_file(model,\n",
    "                      target_image,\n",
    "                      output_image,\n",
    "                      custom_test_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Visualizing outputs for Bangkok and Ubon-Ratchathani\n",
    "import os\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Step 1: read the raster file\n",
    "# IMPORTANT: Make sure that the paths contain the output inference images\n",
    "cm_outpath = \"output/inference_outputs/chiang-mai_18bands_pred.tif\"\n",
    "kk_outpath = \"output/inference_outputs/khon-kaen_18bands_pred.tif\"\n",
    "st_outpath = \"output/inference_outputs/sura-thani_18bands_pred.tif\"\n",
    "\n",
    "def visualize_lu_raster(raster_path,region):\n",
    "    # Step 1: read the file\n",
    "    with rio.open(raster_path) as src:\n",
    "        raster_data = src.read(1) #read just the 1st band\n",
    "        crs = src.crs\n",
    "\n",
    "    print(\"Raster CRS:\",crs) #check the coordinate reference system of the raster file\n",
    "\n",
    "    # Step 2: define the colormap\n",
    "    # Colors are assigned based on value / Landuse type\n",
    "    value_to_color = {\n",
    "        1:'red',\n",
    "        2:'yellow',\n",
    "        3:'green',\n",
    "        4:'blue',\n",
    "        5:'purple',\n",
    "        6:'pink'}\n",
    "\n",
    "    # create a colormap with these values\n",
    "    colors = [value_to_color.get(i,'black') for i in range(0,max(value_to_color.keys()) + 1)]\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    # Step 3: Display the raster with the colormap\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f\"{region} Use Map\")\n",
    "    plt.imshow(raster_data,cmap=cmap,interpolation='none')\n",
    "\n",
    "    # Step 4: Add legend\n",
    "    # define the raster value and corresponding land use type\n",
    "    lu_legend = {1:'Urban',\n",
    "                 2:'Agricultural',\n",
    "                 3:'Forest',\n",
    "                 4:'Water',\n",
    "                 5:'Oil Palm',\n",
    "                 6:'Para Rubber'}\n",
    "\n",
    "    legend_elements = [Patch(facecolor=color, edgecolor='black', label=lu_legend[value])\n",
    "                       for value, color in value_to_color.items()]\n",
    "    plt.legend(handles=legend_elements, title=\"Legend\", loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "    # show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "## visualize the outputs of inferencing\n",
    "visualize_lu_raster(cm_outpath,\"Chiang Mai\")\n",
    "visualize_lu_raster(kk_outpath,\"Khon Kaen\")\n",
    "visualize_lu_raster(st_outpath,\"Sura Thani\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Reference model still to be replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
