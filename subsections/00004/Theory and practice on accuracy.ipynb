{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed78f3f-b348-47f9-b9ee-81b1bac10102",
   "metadata": {},
   "source": [
    "## 4.1.1.2 Theory and practice on accuracy assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97ce83-4bb7-47c1-8a02-b1e44f8c91ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [Accuracy Assessment](https://egyankosh.ac.in/bitstream/123456789/39544/1/Unit-14.pdf)\n",
    "\n",
    "### Concept\n",
    "\n",
    "Accuracy assessment is the final step in the analysis of remote sensing data which help us to verify how accurate our results are. It is carried out once the interpretation/classification has been completed. Here, we are interested in assessing accuracy of thematic maps or classified images which is known as thematic or classification accuracy. The accuracy is concerned with the correspondence between class label and ‘true’ class. A ‘true’ class is defined as what is observed on the ground during field surveys. For example, a class labeled as water on a classified image/map is actually water on the ground.\n",
    "To perform an accuracy assessment, we compare:\n",
    " - Interpreted map/classified image from remote sensing data.\n",
    " - Reference map, high-resolution images, or ground truth data.\n",
    "\n",
    "This relationship is expressed through:\n",
    " - Error Matrix: Compares the two information sources.\n",
    " - Kappa Coefficient: Measures the agreement between rows and columns of the error matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448e7b7-d09d-44a2-90a8-5c892cb4efdf",
   "metadata": {},
   "source": [
    "### Definition\n",
    "Accuracy in the context of image interpretation refers to the quality of information derived from remote sensing data. This can be assessed qualitatively by comparing map images to actual ground views to see if they \"look right.\" Quantitative assessments, however, involve identifying and measuring map errors by comparing map data with ground truth data, assumed to be 100% correct. Image classification accuracy is often reported as a percentage and includes consumer's accuracy (CA) and producer's accuracy (PA). CA is calculated by comparing correctly classified pixels to the total pixels assigned to a category, accounting for errors of commission. It takes errors of commission into account by telling the consumer that, for all areas identified as category X, a certain percentage are actually correct. PA measures the percentage of correctly classified pixels in a category relative to the total actual pixels in that category. Producer’s accuracy measures errors of omission.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd67b2c-01fa-403d-8a5f-25cb10d94141",
   "metadata": {},
   "source": [
    "#### Calculation of Classification Accuracy\n",
    "\n",
    "Accuracy is often expressed in terms of consumer’s and producer’s accuracies which is obtained from an error matrix.\n",
    "\n",
    "An error matrix, also known as a confusion matrix, is a powerful tool used in the field of remote sensing to assess the accuracy of a classified image. It provides a detailed comparison between the classified data and ground truth data, helping to quantify how well the classification process has performed. \n",
    "\n",
    "Here’s a breakdown of its key components and how it’s used:\n",
    "\n",
    "##### Structure of an Error Matrix\n",
    "The error matrix is typically presented in a square table format where:\n",
    "\n",
    " - Rows represent the actual ground truth classes.\n",
    " - Columns represent the predicted classes from the classification.\n",
    "\n",
    "Each cell in the matrix indicates the number of pixels that were classified into a particular category. For example, the cell in the first row and first column shows the number of pixels that were correctly classified as class 1 (true positives for class 1), while the cell in the first row and second column shows the number of pixels that were incorrectly classified as class 2 (false positives for class 1).\n",
    "\n",
    "##### Key Metrics Derived from the Error Matrix\n",
    " - Overall Accuracy:\n",
    "\n",
    "Overall accuracy is calculated by dividing the number of correctly classified pixels (sum of the diagonal elements) by the total number of pixels.\n",
    "\n",
    " - Producer’s Accuracy (PA):\n",
    "\n",
    "Producer’s accuracy, or recall, measures the proportion of actual pixels correctly classified for each class. It is calculated for each class by dividing the number of correctly classified pixels by the total number of pixels in that class (row sum).\n",
    "\n",
    " - Consumer’s Accuracy (CA):\n",
    "\n",
    "Consumer’s accuracy, or precision, measures the proportion of pixels classified correctly out of the total classified pixels for each class (column sum).\n",
    "\n",
    " - Kappa Coefficient:\n",
    "\n",
    "The Kappa coefficient (κ) measures the agreement between the classification and the ground truth, correcting for chance agreement. It ranges from -1 to 1, where 1 indicates perfect agreement and 0 indicates no agreement beyond chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d228c-0e28-4b2c-b4bd-bf5a367983f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec0a42-b1a1-42db-a45c-b4d427d7b15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
