# 4.1 Enable designing quality control of geospatial and satellite data
This section focuses on establishing quality control procedures to ensure the reliability and accuracy of geospatial and satellite data used in riverbank erosion modeling.

## Key learning objectives:
-  4.1.1 Types and definitions of geospatial datasets.
-  4.1.2 Theory and practice on accuracy assessment.
-  4.1.3 Calibration and validation (CAL/VAL) of remote sensing data.
-  4.1.4 Use case(s) of quality control of geospatial and satellite data.

## 4.1.1 Types and definitions of geospatial datasets
### a. (Basic) [Spatial data](https://pygis.io/docs/c_features.html)
Creating and understanding spatial data is often a vital component in the field of geospatial data analysis. This lesson introduces you to the process of generating new spatial data from scratch, as well as enhancing your understanding of the structure and manipulation of spatial data in Python.

-  [Hands-on with codes for undestanding spatial data](code/4.1.1spatial_data.ipynb)

### b. [Vector data](https://docs.qgis.org/3.34/en/docs/gentle_gis_introduction/vector_data.html)
Vector features can be decomposed into three different geometric primitives: points, polylines and polygons.

- **Point:** A point is composed of one coordinate pair representing a specific location in a coordinate system. Points are the most basic geometric primitives having no length or area.
- **Line String:** Lines usually represent features that are linear in nature. For example, maps show rivers, roads, and pipelines as vector lines.
- **Polygon:** Polygon features are enclosed areas like dams, islands, country boundaries and so on. Like polyline features, polygons are created from a series of vertices that are connected with a continuous line. 

#### Vector data file formats
- **GeoJSON:** GeoJSON is an open standard format specifically designed for representing simple geographical features, along with their non-spatial attributes, based on JavaScript Object Notation (JSON). It can accommodate diverse types of geographical features such as points, line strings, polygons, and multi-part collections of these types. One of its primary advantages is its readability - both by humans and machines, as it stores all the relevant data in a single text file.
- **GeoPackage:** GeoPackage is a modern, open standard data format that adopts the principles of open format standards, ensuring non-proprietary access and wide compatibility. It leverages SQLite, a widely used, self-contained, and serverless transactional SQL database engine, making it an exceptionally portable and lightweight choice for data storage.
- **Shapefile:** A Shapefile is akin to a feature class – it stores a set of features that share a common geometry type (point, line, or polygon), possess the same attributes, and occupy a shared spatial extent.


#### [Raster Data](https://docs.qgis.org/2.18/en/docs/gentle_gis_introduction/raster_data.html)

A raster dataset is composed of rows (running across) and columns (running down) of pixels (also know as cells). Each pixel represents a geographical region, and the value in that pixel represents some characteristic of that region.

=======
### b. [Raster data](https://docs.qgis.org/3.34/en/docs/gentle_gis_introduction/vector_data.html)

Raster data models use a grid of cells, also known as pixels, to represent real-world objects. These data models are often used to represent and manage a variety of entities such as imagery, surface temperatures, digital elevation models, and more.

#### Raster Data File Formats
- **GeoTiff:** GeoTIFF is a widely-used raster data format in the public domain. It is a great choice when you prioritize portability and platform independence as it embeds spatial information within the TIFF image file itself.

**Optional resources** 
#### [OSM DATA](https://pygis.io/docs/d_access_osm.html)
OpenStreetMap (OSM) is a global collaborative (crowd-sourced) dataset and project that aims at creating a free editable map of the world containing a lot of information about our environment. It contains data for example about streets, buildings, different services, and landuse.

-  [Hands-on with codes for Accessing OSM data in Python](code/4.1.1access_osm.ipynb)

#### [CENSUS DATA](https://pygis.io/docs/d_access_census.html)
Census data provides detailed demographic information such as population density, age distribution, and household income. When combined with remote sensing data, it can help in analyzing and visualizing population patterns and trends over geographical areas.

-  [Hands-on with codes for accessing census and ACS data in Python](code/4.1.1access_census.ipynb)

#### [Point cloud](https://flyguys.com/point-cloud/#:~:text=A%20point%20cloud%20is%20a%20collection%20of%20individual,it%20creates%20a%20three%20dimensional%20map%20or%20model.)

A point cloud is a collection of individual data points in a three-dimensional plane with each point having a set coordinate on the X, Y, and Z axis. When each point is placed together, it creates a three dimensional map or model. For example, if the data compiled to create the point cloud is taken from a neighborhood, it will show the location of each building, tree, and power line, along with its elevation relative to the ground. Depending on the data gathered and how dense the point cloud is, the more details and specific types of terrain and structures you’ll see.

Point clouds are generally created in one of two ways, LiDAR and photogrammetry, and are georeferenced for accuracy.

- LiDAR
Light Detection and Ranging, or LiDAR, is a remote sensing method that scans an area with a laser and then measures the light’s reflection and range from the earth’s surface. This data is cross-referenced with GPS and inertial measurement units and each pulse of the scanner creates a data point. Because laser scanners can have a scan speed of well over a million points per second, LiDAR is the most accurate, detailed, and precise way to create a point cloud.

- Photogrammetry
Photogrammetry creates measurements using a series of images. The photographs are taken from different locations, and points are triangulated then plotted on the three dimensional space. Drones can take thousands of pictures of an area from different angles and analysts can process the images together to develop a point cloud, filling minimal gaps with surface reconstruction.

## 4.1.2 Theory and practice on accuracy assessment
### a. [Accuracy assessment](https://egyankosh.ac.in/bitstream/123456789/39544/1/Unit-14.pdf)
#### Definition
Accuracy in the context of image interpretation refers to the quality of information derived from remote sensing data. This can be assessed qualitatively by comparing map images to actual ground views to see if they "look right." Quantitative assessments, however, involve identifying and measuring map errors by comparing map data with ground truth data, assumed to be 100% correct. Image classification accuracy is often reported as a percentage and includes consumer's accuracy (CA) and producer's accuracy (PA). CA is calculated by comparing correctly classified pixels to the total pixels assigned to a category, accounting for errors of commission. It takes errors of commission into account by telling the consumer that, for all areas identified as category X, a certain percentage are actually correct. PA measures the percentage of correctly classified pixels in a category relative to the total actual pixels in that category. Producer’s accuracy measures errors of omission.

#### Concept
Accuracy assessment is the final step in the analysis of remote sensing data which help us to verify how accurate our results are. It is carried out once the interpretation/classification has been completed. Here, we are interested in assessing accuracy of thematic maps or classified images which is known as thematic or classification accuracy. The accuracy is concerned with the correspondence between class label and ‘true’ class. A ‘true’ class is defined as what is observed on the ground during field surveys. For example, a class labeled as water on a classified image/map is actually water on the ground.
To perform an accuracy assessment, we compare:
 - Interpreted map/classified image from remote sensing data.
 - Reference map, high-resolution images, or ground truth data.

This relationship is expressed through:
 - Error Matrix: Compares the two information sources.
 - Kappa Coefficient: Measures the agreement between rows and columns of the error matrix.

#### Calculation of Classification Accuracy
Accuracy is often expressed in terms of consumer’s and producer’s accuracies which is obtained from an error matrix.
An error matrix, also known as a confusion matrix, is a powerful tool used in the field of remote sensing to assess the accuracy of a classified image. It provides a detailed comparison between the classified data and ground truth data, helping to quantify how well the classification process has performed. 

Here’s a breakdown of its key components and how it’s used:

###### Structure of an Error Matrix
The error matrix is typically presented in a square table format where:
 - Rows represent the actual ground truth classes.
 - Columns represent the predicted classes from the classification.

Each cell in the matrix indicates the number of pixels that were classified into a particular category. For example, the cell in the first row and first column shows the number of pixels that were correctly classified as class 1 (true positives for class 1), while the cell in the first row and second column shows the number of pixels that were incorrectly classified as class 2 (false positives for class 1).

##### Key Metrics Derived from the Error Matrix
 - Overall Accuracy: Overall accuracy is calculated by dividing the number of correctly classified pixels (sum of the diagonal elements) by the total number of pixels.
 - Producer’s Accuracy (PA): Producer’s accuracy, or recall, measures the proportion of actual pixels correctly classified for each class. It is calculated for each class by dividing the number of correctly classified pixels by the total number of pixels in that class (row sum).
 - Consumer’s Accuracy (CA): Consumer’s accuracy, or precision, measures the proportion of pixels classified correctly out of the total classified pixels for each class (column sum).
 - Kappa Coefficient: The Kappa coefficient (κ) measures the agreement between the classification and the ground truth, correcting for chance agreement. It ranges from -1 to 1, where 1 indicates perfect agreement and 0 indicates no agreement beyond chance.

**Other resources**
### b. (Basic) [Accuracy assessment](https://ebooks.inflibnet.ac.in/geop10/chapter/accuracy-assessment/)
Accuracy is defined as “correctness” i.e. it measures the agreement between a Standard assumed to be correct and a classified image of unknown quality. An accuracy assessment compares two sources of information: 1) pixels or polygons from a classification map developed from remotely sensed data and 2) ground reference test information, if the image classification corresponds closely with the standard, it is said to be “accurate”. 

## 4.1.3 Calibration and validation (CAL/VAL) of remote sensing data
### a. (Basic) [The ultimate geospatial data validation checklist](https://fme.safe.com/blog/2014/11/data-quality-checklist/)
A data quality checklist that can help you validate and repair your geospatial data. Validation steps will vary to some extent depending on the type of data (2D, GIS, raster, etc.), but this list will provide a good guideline. The example of raster data check show in below:

#### [Check the coordinate system](https://chatgpt.com/share/6172ce4e-d071-4579-b638-75d912305eda)
Ensure that the coordinate system (e.g., WGS84, NAD83) of your data matches the coordinate system expected by the destination system. Misaligned coordinate systems can result in incorrect data positioning, which can be critical in spatial data applications.

#### [Why check the projection?](https://www.linkedin.com/advice/3/how-can-you-check-projection-your-shapefile-qgis-before#:~:text=Then%2C%20right%2Dclick%20on%20the,and%20code%20of%20the%20projection.)

The projection of a vector/raster layer determines how it relates to other spatial data and the real world. If you use vector/raster layer with different projections in the same map or analysis, you may encounter errors, misalignments, or inconsistencies. For example, if you overlay a shapefile (vector layer) of countries in a Mercator projection, which preserves angles but exaggerates areas, with a shapefile of population density in an equal-area projection, which preserves areas but distorts shapes, you may get misleading results. Therefore, you should always check the projection of your vector/raster layer before using them, and reproject them if necessary.

#### How to check the projection in QGIS?
 - Right-click on the layer (raster or vector layer) in the Layers panel and select Properties.
 - In the Properties window, click on the Information tab and look for the Coordinate Reference System (CRS) section to view the name and code of the projection.

**Optinal resources** 
### b. (Advanced) [Landsat geometric validation](https://www.usgs.gov/calval)
Accurate geometry ensures that Landsat data pixels are aligned, and that the data can be used easily in time series analysis. The exceptional geometric qualities of Landsat 8 and Landsat 9 OLI/TIRS data are used to improve the reference database used to precisely and accurately geolocate all Landsat Level-1 data products. Landsat Science products inherit the geometry of Landsat Level-1 data.  Validating the geometric accuracy of Landsat data incorporates the known processing levels, the root-mean-square error (RMSE) information, and the use of Ground Control Points

### c. (Advanced) [What are the most important techniques for calibrating remote sensing data?](https://www.linkedin.com/advice/1/what-most-important-techniques-calibrating-wkobc#:~:text=Radiometric%20calibration%20is%20the%20process,scattering%2C%20absorption%2C%20and%20haze.)
Calibrating remote sensing data is a crucial step for ensuring the accuracy and reliability of the information derived from satellite or aerial images. Remote sensing data can be affected by various sources of error, such as atmospheric conditions, sensor characteristics, geometric distortions, and radiometric variations.
The information below show some of the most important techniques for calibrating remote sensing data and how they can improve your GIS analysis.

 - Radiometric calibration
Radiometric calibration adjusts the pixel values of remote sensing data to accurately reflect the true radiance or reflectance of the Earth's surface. This process is crucial for comparing data from different sensors, dates, or locations, and for mitigating the effects of atmospheric scattering, absorption, and haze. Radiometric calibration can be performed using different methods, such as dark object subtraction, histogram matching, empirical line fitting, or radiative transfer models.

 - Geometric calibration
Geometric calibration is the process of correcting spatial distortions in remote sensing data caused by factors such as sensor orientation, platform motion, terrain relief, and map projection. This calibration is essential for accurately aligning data with other spatial layers, measuring distances and areas, and identifying features and patterns. THe example methods for geometric calibration is orthorectification, image registration, resampling, and geometric transformation.

 - Spectral calibration
Spectral calibration is the process of adjusting the spectral characteristics of remote sensing data to match the spectral response of the target objects or materials. This is important for identifying and classifying land cover types, vegetation types, mineral types, or water quality parameters. Spectral calibration can be performed using different methods, such as spectral matching, spectral unmixing, spectral normalization, or spectral enhancement.

 - Temporal calibration
Temporal calibration is the process of adjusting the temporal resolution of remote sensing data to match the temporal dynamics of the phenomena or processes of interest. This is important for monitoring changes, trends, and anomalies over time, and for analyzing seasonal, annual, or long-term variations. Temporal calibration can be performed using different methods, such as temporal interpolation, temporal aggregation, temporal smoothing, or temporal filtering.

 - Calibration tools
Calibration tools are the software applications or modules that facilitate the implementation of the calibration techniques described above. They can be integrated into GIS software or standalone programs that can handle various types of remote sensing data and formats. Some examples of calibration tools are ENVI, ERDAS IMAGINE, ArcGIS Image Analyst, QGIS, or R.

## 4.1.4 Accuracy assessment (classification accuracy)
### a. (Code) [Machine learning with Google Earth Engine - Accuracy assessment](https://github.com/gee-community/geemap/blob/master/examples/notebooks/33_accuracy_assessment.ipynb)
It is important to get a quantitative estimate of the accuracy of the classification. To do this, a common strategy is to divide your training samples into 2 random fractions - one used for training the model and the other for validation of the predictions. Once a classifier is trained, it can be used to classify the entire image. We can then compare the classified values with the ones in the validation fraction

-  [Hands-on with codes for Accuracy assessment](code/4.1.1accuracy_assessment.ipynb)

## 4.1.5 Geospatial data assessment
### a. (Basic) [How to identify unknown coordinate systems for shapefiles](https://mapscaping.com/identifying-coordinate-systems-for-shapefiles/)
Coordinate systems are the foundation for understanding and working with geospatial data, as they provide a standardized way to represent geographic locations on the Earth’s surface. When a shapefile is missing its coordinate system, it can lead to misaligned layers, incorrect measurements, and a host of other issues that hinder accurate data visualization and analysis.

#### Here’s a step-by-step process to help you identify the coordinate system:
**Check metadata and accompanying files**:

When working with a shapefile, it’s essential to check its metadata and accompanying files to gain insights into the dataset, such as its coordinate system, attribute information, and data sources. Here’s how to do it:

1.  Check accompanying files: A shapefile consists of several mandatory and optional files, including:
    - shp: The main file containing geometry data.
    - shx: The index file for quick access to geometry data.
    - dbf: The dBASE table containing attribute data.
    - prj: The projection file containing the coordinate system information (if available)
2.  Examine the .prj file: The .prj file stores the projection and coordinate system information for a shapefile. If this file is missing or empty, you’ll need to identify the coordinate system yourself. If the file is present and populated, you can open it with a text editor to view the [Well-Known Text (WKT) description of the coordinate system](https://mapscaping.com/beginners-guide-to-srid/).
3.  Explore attribute data: Open the attribute table of the shapefile in your GIS software (e.g., QGIS, ArcGIS) to examine the attribute data. This may give you additional context about the dataset, including its source, collection methods, or other relevant information.
4.  Review any accompanying documentation: Sometimes, shapefiles come with accompanying documentation files, like a README.txt or a PDF report, that provide information about the dataset. Make sure to review these documents for any details about the coordinate system, data source, or other important information.
5.  Reach out to the data provider: If you cannot find the information you need in the metadata or accompanying files, consider reaching out to the data provider or source. They may be able to provide the necessary information, such as the coordinate system or data collection methods.

By thoroughly checking the metadata and accompanying files of a shapefile, you can gain valuable insights that will help ensure you’re working with the data correctly and producing accurate results in your geospatial analyses.

### b. (Basic) [Validating data using checks](https://desktop.arcgis.com/en/arcmap/latest/extensions/data-reviewer/checks-in-data-reviewer.htm)
ArcGIS Data Reviewer Desktop offers many checks that can be selected from the Select Data Check drop-down list on the Data Reviewer toolbar. These checks allow you to perform geometric and attribute validation as well as ensure data integrity. All the validation checks can be applied to an entire feature class or database, features within the current extent, or only the selected set of features. 
Below are the categories in which the checks are grouped with a brief description of each individual check.
 
- Database Validation checks
- Default checks
- Duplicate Geometry checks
- Event checks
- Feature on Feature checks
- Polygon checks
- Polyline checks
- Spatial Parameter Evaluation checks
- Table checks
- Z Value checks
- Advanced checks

### c. (Basic) [Geometry validation](https://desktop.arcgis.com/en/arcmap/latest/manage-data/using-sql-with-gdbs/geometry-validation.htm)

Validation rules for point shapes:
 - The area and length of points are 0.0.
 - A single point's envelope is equal to the point's x,y values.
 - The envelope of a multipart point shape is the minimum bounding box.

Validation rules for simple lines or linestrings:
 - Sequential duplicate points are deleted.
 - Each part must have at least two distinct points.
 - Each part may not intersect itself. The start and end points can be the same, but the resulting ring is not treated as an area shape.
 - Parts may touch each other at the end points.
 - The length is the sum of all the parts.
 - Sequential duplicate points are deleted

Validation rules for lines or spaghetti strings:
 - Lines can intersect themselves.
 - Each part must have at least two distinct points.
 - Sequential duplicate points are deleted.
 - The length is the sum of all the parts.

Validation rules and operations for area shapes:
- Duplicate sequential occurrences of a coordinate point are deleted.
- Dangles are deleted.
- Line segments are verified to be closed (z-coordinates at start and end points must also be the same) and don't cross.
- For area shapes with holes, holes must reside wholly inside the outer boundary. ArcGIS eliminates any holes that are outside the outer boundary.
- A hole that touches an outer boundary at a single common point is converted into an inversion of the area shape.
- Multiple holes that touch at common points are combined into a single hole.
- Multipart area shapes cannot overlap. However, two parts may touch at a point.
- Multipart area shapes cannot share a common boundary. Common boundaries are dissolved.
- If two rings have a common boundary, they are merged into one ring.
- The total geometry perimeter, including the boundaries of all holes in donut polygons, is calculated and stored as the length of the geometry.
- etc.

### (Basic) [How to validate & fix geometry in QGIS](https://www.youtube.com/watch?v=PUrSEnEkNPQ)
Demonstrate how to validate and fix some polygon geometry, this video will show you two key features of QGIS.


