{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cbWzUGGak7u"
   },
   "source": [
    "# Object Detection with Yolov3\n",
    "\n",
    "\n",
    "![cover](https://bitmovin.com/wp-content/uploads/2019/08/Object_detection_Blog_Image_Q3_19.jpg)\n",
    "\n",
    "Object detection is a computer vision task that involves both localizing one or more objects within an image and classifying each object in the image.\n",
    "\n",
    "It is a challenging computer vision task that requires both successful object localization in order to locate and draw a bounding box around each object in an image, and object classification to predict the correct class of object that was localized.\n",
    "Yolo is a faster object detection algorithm in computer vision and first described by Joseph Redmon, Santosh Divvala, Ross Girshick and Ali Farhadi in ['You Only Look Once: Unified, Real-Time Object Detection'](https://arxiv.org/abs/1506.02640)\n",
    "\n",
    "This notebook implements an object detection based on a pre-trained model - [YOLOv3 Pre-trained Weights (yolov3.weights) (237 MB)](https://pjreddie.com/media/files/yolov3.weights).  The model architecture is called a “DarkNet” and was originally loosely based on the VGG-16 model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6s9X5c_0kHg"
   },
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (4.21.12)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (2.28.2)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (60.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (1.54.3)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorflow) (1.26.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: rich in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.4.2)\n",
      "Requirement already satisfied: namex in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 08:32:56.185540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-10 08:32:57.292647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-10 08:32:57.296940: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-10 08:32:59.953645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 08:33:09.105980: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Kyn266nShw1y"
   },
   "outputs": [],
   "source": [
    "#import struct\n",
    "#import numpy as np\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import add, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "from numpy import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yolov3_model():\n",
    "    # Define the YOLOv3 model architecture here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yolo-pyutils in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (0.1.36)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yolo-pyutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo_utils.py\n",
    "\n",
    "def make_yolov3_model():\n",
    "    # Define the YOLOv3 model architecture here\n",
    "    print(\"YOLOv3 model created\")\n",
    "    return \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolo34py\n",
      "  Using cached yolo34py-0.2.tar.gz (67 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cython>=0.27 (from yolo34py)\n",
      "  Using cached Cython-3.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from yolo34py) (2.28.2)\n",
      "Requirement already satisfied: numpy in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from yolo34py) (1.26.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests->yolo34py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests->yolo34py) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests->yolo34py) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/my-conda-envs/py310/lib/python3.10/site-packages (from requests->yolo34py) (2023.7.22)\n",
      "Using cached Cython-3.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Building wheels for collected packages: yolo34py\n",
      "  Building wheel for yolo34py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yolo34py: filename=yolo34py-0.2-cp310-cp310-linux_x86_64.whl size=325722 sha256=d25e8260e7dd04c5afd05702f54deb6681fd1ffee977cfd19b8344e8b94e1c2a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/69/e4/a0/1937adc0d8005c439645302e383c3cea81cd05e4d90f4df610\n",
      "Successfully built yolo34py\n",
      "Installing collected packages: cython, yolo34py\n",
      "Successfully installed cython-3.0.10 yolo34py-0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yolo34py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolo34py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolo34py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_yolov3_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolo34py'"
     ]
    }
   ],
   "source": [
    "from yolo34py import make_yolov3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oqGirIFcsd2"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = make_yolov3_model()\n",
    "# load the model weights\n",
    "weight_reader = WeightReader('/home/jovyan/shared/Learning materials/subsections/1.*/code/model/yolov3.weights')\n",
    "# set the model weights into the model\n",
    "weight_reader.load_weights(model)\n",
    "# # save the model to file\n",
    "# model.save('/content/drive/My Drive/DPprojects/Object Detection - Yolo/model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VbUr46tIdGaw",
    "outputId": "99d230b1-73cf-4124-dfe3-cd2dfadd386b"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oo2yI1804Q0"
   },
   "source": [
    "## Make Preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZojkco5A4vz"
   },
   "outputs": [],
   "source": [
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "\t\"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "\t\"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "\t\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "\t\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "\t\"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
    "\t\"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "\t\"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "\t\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "\t\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bluB5mi6iDTX"
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH=416\n",
    "IMAGE_HEIGHT=416\n",
    "def load_and_preprocess_image(path,shape):\n",
    "  image=tf.io.read_file(path)\n",
    "  width,height=load_img(path).size\n",
    "  image=tf.image.decode_jpeg(image,channels=3)\n",
    "  image=tf.image.resize(image, shape)\n",
    "  image/=255\n",
    "  return image,width,height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmoADjikvoEv"
   },
   "source": [
    "Image comes from (https://www.strathcona.ca/transportation-roads/traffic/traffic-signals/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "3GdnPZLnjECD",
    "outputId": "8a44c90a-de95-498d-cd80-ca6374a247cf"
   },
   "outputs": [],
   "source": [
    "photo_filename='/content/drive/My Drive/DPprojects/Object Detection - Yolo/images/traffic.jpg'\n",
    "_image, image_w, image_h=load_and_preprocess_image(photo_filename,[IMAGE_WIDTH,IMAGE_HEIGHT])\n",
    "plt.imshow(_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vZzEwLnFj6cm",
    "outputId": "c89b492b-760d-4dcd-ec68-12cbc1224c88"
   },
   "outputs": [],
   "source": [
    "image = expand_dims(_image, 0)\n",
    "yhat = model.predict(image)\n",
    "print([a.shape for a in yhat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLIu0OAP1ONr"
   },
   "source": [
    "## Process output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNFnKYr6uMAI"
   },
   "outputs": [],
   "source": [
    "# This cell is based on https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
    "class BoundBox:\n",
    "\t'''\n",
    "\tObjects of boxes. (xmin,ymin) represents the upleft coordinate of the box while (xmax,ymax) means downright one.\n",
    "\t'''\n",
    "\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "\t\tself.xmin = xmin\n",
    "\t\tself.ymin = ymin\n",
    "\t\tself.xmax = xmax\n",
    "\t\tself.ymax = ymax\n",
    "\t\tself.objness = objness\n",
    "\t\tself.classes = classes\n",
    "\t\tself.label = -1\n",
    "\t\tself.score = -1\n",
    "\n",
    "\tdef get_label(self):\n",
    "\t\tif self.label == -1:\n",
    "\t\t\tself.label = np.argmax(self.classes)\n",
    "\n",
    "\t\treturn self.label\n",
    "\n",
    "\tdef get_score(self):\n",
    "\t\tif self.score == -1:\n",
    "\t\t\tself.score = self.classes[self.get_label()]\n",
    "\n",
    "\t\treturn self.score\n",
    "\n",
    "def _sigmoid(x):\n",
    "\treturn 1. / (1. + np.exp(-x))\n",
    "\n",
    "def decode_netout(netout, anchors, net_h, net_w):\n",
    "\tgrid_h, grid_w = netout.shape[:2]\n",
    "\tnb_box = 3\n",
    "\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "\tnb_class = netout.shape[-1] - 5\n",
    "\tboxes = []\n",
    "\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "\n",
    "\tfor i in range(grid_h*grid_w):\n",
    "\t\trow = i / grid_w\n",
    "\t\tcol = i % grid_w\n",
    "\t\tfor b in range(nb_box):\n",
    "\t\t\t# 4th element is objectness score\n",
    "\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n",
    "\t\t\t# if(objectness.all() <= obj_thresh): continue\n",
    "\t\t\t# first 4 elements are x, y, w, and h\n",
    "\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "\t\t\tx = (col + x) / grid_w # center position, unit: image width\n",
    "\t\t\ty = (row + y) / grid_h # center position, unit: image height\n",
    "\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
    "\t\t\t# last elements are class probabilities\n",
    "\t\t\tclasses = netout[int(row)][col][b][5:]\n",
    "\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "\t\t\tboxes.append(box)\n",
    "\treturn boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcC3a0tjtYlv"
   },
   "outputs": [],
   "source": [
    "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "\tboxes += decode_netout(yhat[i][0], anchors[i], net_h=IMAGE_HEIGHT, net_w=IMAGE_WIDTH)\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "\tx_offset, x_scale = (IMAGE_WIDTH - IMAGE_WIDTH)/2./IMAGE_HEIGHT, float(IMAGE_WIDTH)/IMAGE_WIDTH\n",
    "\ty_offset, y_scale = (IMAGE_HEIGHT - IMAGE_HEIGHT)/2./IMAGE_HEIGHT, float(IMAGE_HEIGHT)/IMAGE_HEIGHT\n",
    "\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4BVYDz-UBGFP",
    "outputId": "0b35942c-8104-4b5a-b477-e3b060182d98"
   },
   "outputs": [],
   "source": [
    "len(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX39d08LBMNu"
   },
   "source": [
    "## Boxes Filters\n",
    "\n",
    "From the sections above, it is clear to see that there are over ten thousands of boxes detected. This next step is to filter the boxes with very low confidence by using threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tghx5DRpr3hn"
   },
   "outputs": [],
   "source": [
    "def box_filter(boxes,labels,threshold_socre):\n",
    "\tvalid_boxes=[]\n",
    "\tvalid_labels=[]\n",
    "\tvalid_scores=[]\n",
    "\tfor box in boxes:\n",
    "\t\tfor i in range(len(labels)):\n",
    "\t\t\tif box.classes[i] > threshold_socre:\n",
    "\t\t\t\tvalid_boxes.append(box)\n",
    "\t\t\t\tvalid_labels.append(labels[i])\n",
    "\t\t\t\tvalid_scores.append(box.classes[i])\n",
    "\n",
    "\treturn (valid_boxes,valid_labels,valid_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7ewYu44umZ6"
   },
   "outputs": [],
   "source": [
    "valid_data= box_filter(boxes, labels, threshold_socre=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIu6fLbu0l6b"
   },
   "source": [
    "## Draw all the boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpCz8tYK0sh6"
   },
   "outputs": [],
   "source": [
    "def draw_boxes(filename, valid_data):\n",
    "\n",
    "\tdata = pyplot.imread(filename)\n",
    "\tpyplot.imshow(data)\n",
    "\tax = pyplot.gca()\n",
    "\tfor i in range(len(valid_data[0])):\n",
    "\t\tbox = valid_data[0][i]\n",
    "\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "\t\twidth, height = x2 - x1, y2 - y1\n",
    "\t\trect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
    "\t\tax.add_patch(rect)\n",
    "\t\tprint(valid_data[1][i], valid_data[2][i])\n",
    "\t\tlabel = \"%s (%.3f)\" % (valid_data[1][i], valid_data[2][i])\n",
    "\t\tpyplot.text(x1, y1, label, color='white')\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "K1KeBbGLuuQL",
    "outputId": "1bc26826-1fb7-45be-ee65-96d7f6d3d47a"
   },
   "outputs": [],
   "source": [
    "draw_boxes(photo_filename,valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj1x3pGVFqgB"
   },
   "source": [
    "## Non-max Seppression, NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9HqPhnkT6oZ"
   },
   "outputs": [],
   "source": [
    "def encoder_dic(valid_data):\n",
    "  data_dic={}\n",
    "  (valid_boxes,valid_labels,valid_scores)=valid_data\n",
    "  for box, label,score in zip(valid_boxes,valid_labels,valid_scores):\n",
    "    if label not in data_dic:\n",
    "      data_dic[label]=[[score,box,'kept']]\n",
    "    else:\n",
    "      data_dic[label].append([score,box,'kept'])\n",
    "\n",
    "  return data_dic\n",
    "dic=encoder_dic(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kLp8wd6FfP3"
   },
   "outputs": [],
   "source": [
    "def decode_box_coor(box):\n",
    "  return (box.xmin, box.ymin,box.xmax, box.ymax )\n",
    "\n",
    "def iou(box1, box2):\n",
    "  (box1_x1, box1_y1, box1_x2, box1_y2) = decode_box_coor(box1)\n",
    "  (box2_x1, box2_y1, box2_x2, box2_y2) = decode_box_coor(box2)\n",
    "\n",
    "  xi1 = max(box1_x1,box2_x1)\n",
    "  yi1 = max(box1_y1,box2_y1)\n",
    "  xi2 = min(box1_x2,box2_x2)\n",
    "  yi2 = min(box1_y2,box2_y2)\n",
    "  inter_width = xi2-xi1\n",
    "  inter_height = yi2-yi1\n",
    "  inter_area = max(inter_height,0)*max(inter_width,0)\n",
    "\n",
    "  box1_area = (box1_x2-box1_x1)*(box1_y2-box1_y1)\n",
    "  box2_area = (box2_x2-box2_x1)*(box2_y2-box2_y1)\n",
    "  union_area = box1_area+box2_area-inter_area\n",
    "\n",
    "  iou = inter_area/union_area\n",
    "\n",
    "  return iou\n",
    "\n",
    "def do_nms(data_dic, nms_thresh):\n",
    "  final_boxes,final_scores,final_labels=list(),list(),list()\n",
    "  for label in data_dic:\n",
    "    scores_boxes=sorted(data_dic[label],reverse=True)\n",
    "    for i in range(len(scores_boxes)):\n",
    "      if scores_boxes[i][2]=='removed': continue\n",
    "      for j in range(i+1,len(scores_boxes)):\n",
    "        if iou(scores_boxes[i][1],scores_boxes[j][1]) >= nms_thresh:\n",
    "          scores_boxes[j][2]=\"removed\"\n",
    "\n",
    "    for e in scores_boxes:\n",
    "      print(label+' '+str(e[0]) + \" status: \"+ e[2])\n",
    "      if e[2]=='kept':\n",
    "        final_boxes.append(e[1])\n",
    "        final_labels.append(label)\n",
    "        final_scores.append(e[0])\n",
    "\n",
    "\n",
    "  return (final_boxes,final_labels,final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "7tKq4lZYiLfV",
    "outputId": "a3796a0a-a78e-4136-9ab9-d76ec8a42362"
   },
   "outputs": [],
   "source": [
    "final_data=do_nms(dic, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "L6TTEa14iwKJ",
    "outputId": "b1e1198e-58fb-43b4-dc24-ff4dfa18f0de"
   },
   "outputs": [],
   "source": [
    "draw_boxes(photo_filename,final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP6jc8D0lmDu"
   },
   "source": [
    "### Use Keras \"non_max_suppression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6krAd4QxImK4"
   },
   "outputs": [],
   "source": [
    "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
    "\n",
    "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n",
    "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n",
    "    nms_indices = tf.image.non_max_suppression(scores=scores,boxes=boxes,max_output_size=max_boxes,iou_threshold=iou_threshold)\n",
    "\n",
    "    scores = K.gather(scores,nms_indices)\n",
    "    boxes = K.gather(boxes,nms_indices)\n",
    "    classes = K.gather(classes,nms_indices)\n",
    "\n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwYVtNF4l41m"
   },
   "source": [
    "## Other Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-KsBFOSl4Ps"
   },
   "outputs": [],
   "source": [
    "def showresults(path):\n",
    "  _image,width,height=load_and_preprocess_image(path,[IMAGE_WIDTH,IMAGE_HEIGHT])\n",
    "  image = expand_dims(_image, 0)\n",
    "  yhat = model.predict(image)\n",
    "  boxes = list()\n",
    "  for i in range(len(yhat)):\n",
    "\t  boxes += decode_netout(yhat[i][0], anchors[i], net_h=IMAGE_HEIGHT, net_w=IMAGE_WIDTH)\n",
    "  for i in range(len(boxes)):\n",
    "    x_offset, x_scale = (IMAGE_WIDTH - IMAGE_WIDTH)/2./IMAGE_HEIGHT, float(IMAGE_WIDTH)/IMAGE_WIDTH\n",
    "    y_offset, y_scale = (IMAGE_HEIGHT - IMAGE_HEIGHT)/2./IMAGE_HEIGHT, float(IMAGE_HEIGHT)/IMAGE_HEIGHT\n",
    "    boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "    boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "    boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "    boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
    "  valid_data= box_filter(boxes, labels, threshold_socre=0.6)\n",
    "  dic=encoder_dic(valid_data)\n",
    "  final_data=do_nms(dic, 0.7)\n",
    "  draw_boxes(path,final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "8BE3CN9QwzNd",
    "outputId": "4216e2be-1439-450f-8d75-cea804f60ee5"
   },
   "outputs": [],
   "source": [
    "showresults('/content/drive/My Drive/DPprojects/Object Detection - Yolo/images/zebra.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "GEeGlbEeyE0T",
    "outputId": "14c30bd1-91de-4b0d-d7a2-d8a7b6f62dc5"
   },
   "outputs": [],
   "source": [
    "showresults('/content/drive/My Drive/DPprojects/Object Detection - Yolo/images/kangaroo.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LozuraZZ_wnj"
   ],
   "name": "Object Detection - Yolo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
