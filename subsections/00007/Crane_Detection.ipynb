{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Learning objectives\n",
    "\n",
    "This JupyterNotebook aims to learn practice of deep learning for object detection in satellite images, specifically cranes in ports. The contents follow the objectives below.\n",
    "\n",
    "* Understand the basis of deep learning\n",
    "* Practice deep learning for object detection with steps of:\n",
    "    * Train a detection model\n",
    "    * Evaluate model performce\n",
    "    * Improve model training\n",
    "    * Tune a model with new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Introduction to deep learning for object detection\n",
    "##  2.1 What is deep learning?\n",
    "\n",
    "### Introduction to deep learning\n",
    "\n",
    "The term ‘Artificial Intelligence (AI)’ has been introduced for 50 years and still it is in the global trend. These days machine learning (ML) is interchangeably used for AI as it is one of the most popular and successful sub-branches of AI. Deep learning is a technique that uses multi-layered neural networks, to mimic human-like recognition and try to find the most optimal path to a solution.\n",
    "\n",
    "### Deep learning for image recognition\n",
    "\n",
    "Deep Learning has been powerful when it comes to image recognition. Using the neural networks with multiple layers (deep neural networks), these models can automatically learn features and pattern directly from raw image data, significantly outperforming traditional image processing methods.\n",
    "\n",
    "## 2.2 Benefits\n",
    "\n",
    "### Application in practice\n",
    "\n",
    "Deep learning for object detection has a wide range of practical applications, including:\n",
    "* **Automated surveillance:** Enhancing security systems by accurately detecting and classifying objects in real-time.\n",
    "* **Autonomous vehicles:** Enabling self-driving cars to recognize and respond to various objects on the road.\n",
    "* **Medical imaging:** Assisting in the detection of anomalies in medical scans, improving diagnostic accuracy.\n",
    "* **Industrial automation:** Streamlining manufacturing processes by identifying and categorizing different components.\n",
    "* **Remote sensing and earth observation:** Deep learning models can analyze satellite and aerial imagery with high precision, improving environmental monitoring and disaster response. For example, they can detect deforestation, urban expansion, and climate change impacts, as well as rapidly identify areas affected by natural disasters such as floods, hurricanes, and wildfires. This facilitates timely decision-making and efficient resource allocation.\n",
    "\n",
    "## 2.3 Outline of training a detection model\n",
    "\n",
    "Developing a deep leaning model for object detection involves several key steps:\n",
    "\n",
    "*  **Dataset preparation:** Collecting and annotating images relevant to the detection task.\n",
    "*  **Model configuration:** Configure an network to train.\n",
    "*  **Training:** Feeding the annotated dataset into the model training.\n",
    "*  **Evaluation:** Assessing the model's performance using accuracy metrics, such as Mean Average Precision (mAP).\n",
    "*  **Tuning:** Refining the model training through hyperparameter tuning, data augmentation, and other techniques to achieve better performance and generality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prerequisites\n",
    "\n",
    "### 3.1 Preferred skillsets for the following hands-on practice\n",
    "\n",
    "#### Recommended readings for in-depth understanding\n",
    "\n",
    "- [Official documents of MMDetection](https://mmdetection.readthedocs.io/): General information of MMDetection.\n",
    "\n",
    "* [MMDetection Benchmark and Model Zoo](https://mmdetection.readthedocs.io/en/latest/model_zoo.html): Theoretical background of the models and methods.\n",
    "\n",
    "### 3.2 System requirements\n",
    "\n",
    "#### Hardware recommendation\n",
    "\n",
    "* Processor: A modern multi-core processor, such as an Intel Core i7 (8th generation or newer) or an AMD Ryzen 7 (3rd generation or newer)\n",
    "* RAM: 16 GiB\n",
    "* GPU: NVIDIA GeForce GTX 1070 or a more powerful GPU with at least 4 GB of VRAM, preferrably 8 GB.\n",
    "* NVIDIA driver version: 510 or later\n",
    "\n",
    "#### Software Installation\n",
    "\n",
    "**Please skip this section for the installation if you use GLODAL's JupyterHub environment.**\n",
    "\n",
    "* [Anaconda/ Miniconda](https://docs.anaconda.com/miniconda/miniconda-install/)\n",
    "* [Install JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)\n",
    "* [Install git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
    "\n",
    "This notebook demonstrates the steps to finetune a model for crane detection. We start by setting up the environment, preparing the dataset, and then proceed to train and evaluate the model. Working with the proper versions of packages and libraries is essential. The environment can be set up with the following codes. First, we create the environment `craneDetection` where all required packages will be installed.\n",
    "\n",
    "### Before Installing \n",
    "Make sure the current directory is set to home, running \"cd\".\n",
    "\n",
    "```\n",
    "%cd\n",
    "```\n",
    "\n",
    "#### Installing Required Packages\n",
    "First, ensure that you have Miniconda installed. Then, create a new Conda environment and install the necessary libraries:\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create a new conda environment\n",
    "conda create -n craneDetection python==3.10.12\n",
    "\n",
    "# Activate the newly created environment\n",
    "conda activate craneDetection\n",
    "\n",
    "# Install PyTorch and CUDA\n",
    "conda install -n base -c conda-forge mamba\n",
    "mamba install -c conda-forge cudatoolkit=11.8 cudnn\n",
    "pip install torch==2.1.1 torchvision==0.16.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install MMDetection dependencies\n",
    "pip install -U openmim==0.3.9\n",
    "mim install mmengine==0.9.1\n",
    "pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n",
    "\n",
    "# Install additional packages\n",
    "pip install labelme\n",
    "pip install -U labelme2coco\n",
    "\n",
    "# Clone the mmdetection repository and install it\n",
    "conda install anaconda::git\n",
    "git clone https://github.com/open-mmlab/mmdetection.git\n",
    "cd mmdetection\n",
    "pip install -e .\n",
    "\n",
    "# Install ipykernel\n",
    "conda install anaconda::ipykernel\n",
    "python -m ipykernel install --user --name craneDetection --display-name \"Crane Detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom Loss and Hook\n",
    "\n",
    "As of August 2024, the current version of  **MMDetection (3.3.0)** does not provide validation loss during training, requring a custom hook for this purpose. It needs minor adjustments to libraries and frameworks, including changes to installed packages such as **MMDetection** and **MMEngine**.\n",
    "\n",
    "- Custom loss functions are essential for training models in tasks like object detection. MMDetection allow for tailored loss functions that address specific needs, enhancing model performance and accuracy. [This paper is useful for theoretical overview of loss functions in deep learning](https://arxiv.org/abs/2307.02694).\n",
    "- A logger hook is a vital tool for monitoring and recording the training process. It helps track various metrics, including custom losses, enabling better analysis and debugging of the model's performance. Implementing a custom logger hook ensures that all relevant information is captured during training.\n",
    "\n",
    "To apply the **Custom Hook**, the **MMEngine** library and the **MMDetection** folder must be updated.\n",
    "\n",
    "* 1. Locate the **MMEngine** path. Since this folder is part of a library, its location is hidden. To access and find the location, use the ```**%pip show mmengine**``` command.\n",
    "\n",
    "    After running the ```**%pip show mmengine**``` command, the output should be similar to this:\n",
    "    ```bash\n",
    "        Name: mmengine\n",
    "        Version: 0.9.1\n",
    "        Summary: Engine of OpenMMLab projects\n",
    "        Home-page: https://github.com/open-mmlab/mmengine\n",
    "        Author: MMEngine Authors\n",
    "        Author-email: openmmlab@gmail.com\n",
    "        License: UNKNOWN\n",
    "        Location: /opt/conda/lib/python3.10/site-packages\n",
    "        Requires: addict, matplotlib, numpy, opencv-python, pyyaml, rich, termcolor, yapf\n",
    "        Required-by: mmcv\n",
    "        Note: you may need to restart the kernel to use updated packages.\n",
    "    ```\n",
    "\n",
    "    Now, locate the ***Location*** line in the output. Copy the location path, for example:\n",
    "    ```bash\n",
    "    Location: /opt/conda/lib/python3.10/site-packages\n",
    "    ```\n",
    "     Copy the location path, for example:\n",
    "     ```bash\n",
    "    /opt/conda/lib/python3.10/site-packages\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "    **Note**: The % symbol indicates that this command should be run in a cell or notebook. Use ```pip show mmengine``` for terminal commands, and ```%pip show mmengine``` for cell or notebook commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show mmengine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. Once the location is provided, copy the location path and paste it into the variable ***source_path***. This cell will then copy the files that need to be updated. The files to be copied are:\n",
    "    * ***runtime_info_hook.py***\n",
    "    * ***logger_hook.py***\n",
    "\n",
    "    **Note**: The libraries os and shutil are used for joining paths and copying files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paste the location obtained from the `%pip show mmengine` command into this variable\n",
    "source_path = \"/opt/conda/lib/python3.10/site-packages\" \n",
    "destination_path = \"./files\"  # Create a folder for the files to be copied into\n",
    "\n",
    "\n",
    "os.makedirs(destination_path, exist_ok=True)  # Create the destination directory if it doesn't exist\n",
    "runtime_info_hook_path = os.path.join(source_path, 'mmengine', 'hooks', 'runtime_info_hook.py') # Define the full path to the runtime_info_hook.py file\n",
    "logger_hook_path = os.path.join(source_path, 'mmengine', 'hooks', 'logger_hook.py') # Define the full path to the logger_hook.py file\n",
    "\n",
    "shutil.copy(runtime_info_hook_path, os.path.join(destination_path, 'runtime_info_hook.py')) # Copy runtime_info_hook.py from the source path to the destination folder\n",
    "shutil.copy(logger_hook_path, os.path.join(destination_path, 'logger_hook.py')) # Copy logger_hook.py from the source path to the destination folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Inside the folder \"files,\" there should be 2 files: ***runtime_info_hook.py*** and ***logger_hook.py***.\n",
    "\n",
    "    * Open ***runtime_info_hook.py***. Inside this file, locate the class definition \"**class RuntimeInfoHook(Hook)**\":\n",
    "    ```python\n",
    "    @HOOKS.register_module()\n",
    "    class RuntimeInfoHook(Hook):\n",
    "        \"\"\"A hook that updates runtime information into message hub.\n",
    "\n",
    "        E.g. ``epoch``, ``iter``, ``max_epochs``, and ``max_iters`` for the\n",
    "        training state. Components that cannot access the runner can get runtime\n",
    "        information through the message hub.\n",
    "        \"\"\"\n",
    "\n",
    "        priority = 'VERY_HIGH'\n",
    "    ```\n",
    "\n",
    "    * Inside this class, find the **before_val** function. It should be located between lines 120 to 140 and should look like this:\n",
    "    ```python\n",
    "    def before_val(self, runner) -> None:\n",
    "        self.last_loop_stage = runner.message_hub.get_info('loop_stage')\n",
    "        runner.message_hub.update_info('loop_stage', 'val')\n",
    "    ```\n",
    "\n",
    "    * Below this function, add the following **custom function**:\n",
    "    ```python\n",
    "    ## ADD THIS CUSTOM FUNCTION\n",
    "    def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[dict] = None) -> None:\n",
    "        if outputs is not None:\n",
    "            #print(outputs)\n",
    "            try:\n",
    "                for key, value in outputs.items():\n",
    "                    runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    ```\n",
    "\n",
    "    * The ***runtime_info_hook.py*** file should now look like this:\n",
    "    ```python\n",
    "    def before_val(self, runner) -> None:\n",
    "        self.last_loop_stage = runner.message_hub.get_info('loop_stage')\n",
    "        runner.message_hub.update_info('loop_stage', 'val')\n",
    "        \n",
    "    def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[dict] = None) -> None:\n",
    "        if outputs is not None:\n",
    "            #print(outputs)\n",
    "            try:\n",
    "                for key, value in outputs.items():\n",
    "                    runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    def after_val_epoch(self,\n",
    "                        runner,\n",
    "                        metrics: Optional[Dict[str, float]] = None) -> None:\n",
    "        \"\"\"All subclasses should override this method, if they need any\n",
    "        operations after each validation epoch.\n",
    "\n",
    "        Args:\n",
    "            runner (Runner): The runner of the validation process.\n",
    "            metrics (Dict[str, float], optional): Evaluation results of all\n",
    "                metrics on validation dataset. The keys are the names of the\n",
    "                metrics, and the values are corresponding results.\n",
    "        \"\"\"\n",
    "        if metrics is not None:\n",
    "            for key, value in metrics.items():\n",
    "                if _is_scalar(value):\n",
    "                    runner.message_hub.update_scalar(f'val/{key}', value)\n",
    "                else:\n",
    "                    runner.message_hub.update_info(f'val/{key}', value)\n",
    "    ```\n",
    "\n",
    "    * Now that ***runtime_info_hook.py*** is updated, the next step is to update ***logger_hook.py***.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next, open the file ***logger_hook.py*** located in the **files** folder.\n",
    "\n",
    "    * Inside the file, locate the class **class LoggerHook(Hook)**, which should look like this:\n",
    "    ```python\n",
    "    @HOOKS.register_module()\n",
    "    class LoggerHook(Hook):\n",
    "        \"\"\"Collect logs from different components of ``Runner`` and write them to\n",
    "        terminal, JSON file, tensorboard and wandb .etc.\n",
    "\n",
    "        ``LoggerHook`` is used to record logs formatted by ``LogProcessor`` during\n",
    "        training/validation/testing phase. It is used to control following\n",
    "        behaviors:\n",
    "    ```\n",
    "\n",
    "    * Next, find the function named **after_test_iter**, which should be between lines 220 and 240.\n",
    "    ```python\n",
    "    def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[Sequence] = None) -> None:\n",
    "        \"\"\"Record logs after testing iteration.\n",
    "\n",
    "        Args:\n",
    "            runner (Runner): The runner of the testing process.\n",
    "            batch_idx (int): The index of the current batch in the test loop.\n",
    "            data_batch (dict or tuple or list, optional): Data from dataloader.\n",
    "            outputs (sequence, optional): Outputs from model.\n",
    "        \"\"\"\n",
    "        if self.every_n_inner_iters(batch_idx, self.interval):\n",
    "            _, log_str = runner.log_processor.get_log_after_iter(\n",
    "                runner, batch_idx, 'test')\n",
    "            runner.logger.info(log_str)\n",
    "    ```\n",
    "\n",
    "    * Instead of adding a custom function, replace the existing **after_test_iter** function with the following updated version:\n",
    "    ```python\n",
    "    ###REPLACE THE after_test_iter function\n",
    "    def after_test_iter(self,\n",
    "                            runner,\n",
    "                            batch_idx: int,\n",
    "                            data_batch: DATA_BATCH = None,\n",
    "                            outputs: Optional[dict] = None) -> None:\n",
    "            \"\"\"Record logs after training iteration.\n",
    "\n",
    "            Args:\n",
    "                runner (Runner): The runner of the training process.\n",
    "                batch_idx (int): The index of the current batch in the train loop.\n",
    "                data_batch (dict tuple or list, optional): Data from dataloader.\n",
    "                outputs (dict, optional): Outputs from model.\n",
    "            \"\"\"\n",
    "\n",
    "            runner.logger.setLevel(logging.INFO)\n",
    "            \n",
    "            if self.every_n_train_iters(\n",
    "                    runner, self.interval_exp_name) or (self.end_of_epoch(\n",
    "                        runner.test_dataloader, batch_idx)):\n",
    "                exp_info = f'Exp name: {runner.experiment_name}'\n",
    "                runner.logger.info(exp_info)\n",
    "            if self.every_n_inner_iters(batch_idx, self.interval):\n",
    "                tag, log_str = runner.log_processor.get_log_after_iter(\n",
    "                    runner, batch_idx, 'test')\n",
    "            elif (self.end_of_epoch(runner.test_dataloader, batch_idx)\n",
    "                and (not self.ignore_last\n",
    "                    or len(runner.test_dataloader) <= self.interval)):\n",
    "                tag, log_str = runner.log_processor.get_log_after_iter(\n",
    "                    runner, batch_idx, 'test')\n",
    "            else:\n",
    "                return\n",
    "            \n",
    "            runner.logger.info(log_str)\n",
    "            runner.logger.info(tag)\n",
    "            runner.visualizer.add_scalars(\n",
    "                tag, step=runner.iter + 1, file_path=self.json_log_path)\n",
    "    ```\n",
    "\n",
    "    * Now that both files have been updated, the next step is to overwrite the existing files in the **mmengine** directory using the path obtained earlier from the command ```%pip show mmengine```.\n",
    "    ```bash\n",
    "    \"/opt/conda/lib/python3.10/site-packages\" \n",
    "    ```\n",
    "    Using this path, replace the old files with the updated ones inside the **mmengine** folder.\n",
    "\n",
    "    **Note**: Overwriting files inside the **system's environment** is risky because **site-packages** is where Python packages and libraries installed via pip (or other package managers) are stored. Therefore, **administrative permissions** are required when making changes to files within the **Python packages directory**.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the updated files\n",
    "folder_path = \"./files\"\n",
    "# Define the path to the mmengine directory within the system's environment\n",
    "mmengine_path = \"/opt/conda/lib/python3.10/site-packages\" \n",
    "\n",
    "\n",
    "runtime_info_hook_path = os.path.join(mmengine_path, 'mmengine', 'hooks', 'runtime_info_hook.py') # Define the full path to the runtime_info_hook.py file\n",
    "logger_hook_path = os.path.join(mmengine_path, 'mmengine', 'hooks', 'logger_hook.py') # Define the full path to the logger_hook.py file\n",
    "\n",
    "\n",
    "shutil.copyfile(os.path.join(folder_path, 'runtime_info_hook.py'), runtime_info_hook_path) # Overwrite the existing runtime_info_hook.py file in mmengine with the updated version from the files folder\n",
    "shutil.copyfile(os.path.join(folder_path, 'logger_hook.py'), logger_hook_path) # Overwrite the existing logger_hook.py file in mmengine with the updated version from the files folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After running this cell, the hook files within **mmengine** should be updated, integrating the custom hook. The next step is to update the files inside the **mmdetection** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next, update the files in **mmdetection** and create a new file for the custom hook.\n",
    "\n",
    "    **Note**: If **mmdetection** has not been cloned yet or if the folder is missing (due to skipping the installation or other reasons), use the following command to clone or download **mmdetection** to avoid any errors or missing files.\n",
    "\n",
    "    Before cloning the mmdetection repository double check the current path using \"pwd\"\n",
    "    ```bash\n",
    "    pwd\n",
    "    ``` \n",
    "\n",
    "    If the current location is inside **mmdetection**, go to the home directory using \"cd\"\n",
    "    ```bash\n",
    "    cd\n",
    "    ```\n",
    "\n",
    "    Now proceed on cloning.\n",
    "\n",
    "\n",
    "    ```python \n",
    "    !git clone https://github.com/open-mmlab/mmdetection.git --progress --verbose\n",
    "    ```\n",
    "    The --progress --verbose options provide detailed output during the cloning process. Running this command will clone the repository. If the folder already exists, you may see the following message:\n",
    "    ```python\n",
    "    fatal: destination path 'mmdetection' already exists and is not an empty directory.\n",
    "    ```\n",
    "    This is just a prompt indicating that the folder already exists and won’t cause any issues.\n",
    "\n",
    "    **Note**: When running this command in a notebook cell, the **!** symbol is required to execute it as a shell command. If you’re running the command in a terminal, omit the **!** symbol.\n",
    "\n",
    "    * In a notebook cell:\n",
    "    ```python \n",
    "    !git clone https://github.com/open-mmlab/mmdetection.git --progress --verbose\n",
    "    ```\n",
    "\n",
    "    * In a terminal:\n",
    "    ```python \n",
    "    git clone https://github.com/open-mmlab/mmdetection.git\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/open-mmlab/mmdetection.git --progress --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell should output something similar to this:\n",
    "```python\n",
    "Cloning into 'mmdetection'...\n",
    "POST git-upload-pack (175 bytes)\n",
    "POST git-upload-pack (gzip 3902 to 1906 bytes)\n",
    "remote: Enumerating objects: 38023, done.\n",
    "remote: Counting objects: 100% (2/2), done.\n",
    "remote: Compressing objects: 100% (2/2), done.\n",
    "remote: Total 38023 (delta 0), reused 1 (delta 0), pack-reused 38021 (from 1)\n",
    "Receiving objects: 100% (38023/38023), 63.25 MiB | 16.75 MiB/s, done.\n",
    "Resolving deltas: 100% (26223/26223), done.\n",
    "Updating files: 100% (2443/2443), done.\n",
    "```\n",
    "\n",
    "Now that **mmdetection** is cloned (or if the folder already exists), the next step is to update the necessary files. This process should be quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Inside the ***mmdetection*** folder, navigate to  **mmdetection** -> **mmdet** -> **engine** -> **hooks**.\n",
    "\n",
    "    * Create a new file in this folder called **my_hook.py**. The file should now appear as:\n",
    "    ```bash\n",
    "    my_hook.py\n",
    "    ```\n",
    "\n",
    "    * Since ***my_hook.py*** is currently empty, we will add a custom logger. This custom hook will call the functions that were updated in ***MMEngine*** earlier. Add the*** following script to the ***my_hook.py*** file:\n",
    "\n",
    "    ```python\n",
    "    from mmengine.hooks import Hook\n",
    "    from mmengine.runner import Runner\n",
    "\n",
    "    from mmdet.registry import HOOKS\n",
    "\n",
    "    from mmengine.hooks.logger_hook import LoggerHook\n",
    "    from mmengine.hooks.runtime_info_hook import RuntimeInfoHook\n",
    "\n",
    "\n",
    "    @HOOKS.register_module()\n",
    "    class MyHook(Hook):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def val_step(self, model, data, optim_wrapper):\n",
    "            with optim_wrapper.optim_context(model):\n",
    "                data = model.data_preprocessor(data, True)\n",
    "                losses = model(**data, mode='loss')\n",
    "            parsed_losses, log_vars = model.parse_losses(losses)  \n",
    "            return log_vars\n",
    "\n",
    "        def after_train_epoch(self, runner) -> None:\n",
    "            model = runner.model\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            optim_wrapper = runner.optim_wrapper\n",
    "            dataloader = runner.test_dataloader\n",
    "            for hook in runner._hooks:\n",
    "                if isinstance(hook, LoggerHook):\n",
    "                    logger = hook\n",
    "                elif isinstance(hook, RuntimeInfoHook):\n",
    "                    runtimeinfo = hook\n",
    "\n",
    "            for i, data in enumerate(dataloader):\n",
    "                outputs = self.val_step(model, data, optim_wrapper)\n",
    "                # Ensure that the methods exist and are called with the correct arguments\n",
    "                if hasattr(runtimeinfo, 'after_test_iter'):\n",
    "                    getattr(runtimeinfo, 'after_test_iter')(runner, None, None, outputs)\n",
    "                if hasattr(logger, 'after_test_iter'):\n",
    "                    getattr(logger, 'after_test_iter')(runner, i+1, outputs)\n",
    "    ```\n",
    "\n",
    "    * Now that the custom hook file is complete, the final step is to update the initialization file, **init.py**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Navigate to the ***mmdetection*** path: **mmdetection** -> **mmdet** -> **engine** -> **hooks**. Inside this folder, there should be a file named **__init__.py**. Open this file.\n",
    "\n",
    "    * After openning the file is should like this.\n",
    "    ```python\n",
    "        # Copyright (c) OpenMMLab. All rights reserved.\n",
    "    from .checkloss_hook import CheckInvalidLossHook\n",
    "    from .mean_teacher_hook import MeanTeacherHook\n",
    "    from .memory_profiler_hook import MemoryProfilerHook\n",
    "    from .num_class_check_hook import NumClassCheckHook\n",
    "    from .pipeline_switch_hook import PipelineSwitchHook\n",
    "    from .set_epoch_info_hook import SetEpochInfoHook\n",
    "    from .sync_norm_hook import SyncNormHook\n",
    "    from .utils import trigger_visualization_hook\n",
    "    from .visualization_hook import (DetVisualizationHook,\n",
    "                                    GroundingVisualizationHook,\n",
    "                                    TrackVisualizationHook)\n",
    "    from .yolox_mode_switch_hook import YOLOXModeSwitchHook\n",
    "\n",
    "    __all__ = [\n",
    "        'YOLOXModeSwitchHook', 'SyncNormHook', 'CheckInvalidLossHook',\n",
    "        'SetEpochInfoHook', 'MemoryProfilerHook', 'DetVisualizationHook',\n",
    "        'NumClassCheckHook', 'MeanTeacherHook', 'trigger_visualization_hook',\n",
    "        'PipelineSwitchHook', 'TrackVisualizationHook',\n",
    "        'GroundingVisualizationHook'\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    * Below the line:\n",
    "    ```python\n",
    "    from .yolox_mode_switch_hook import YOLOXModeSwitchHook\n",
    "    ``` \n",
    "\n",
    "    * Add the following import statement to include the custom hook you created earlier (**my_hook.py**). This ensures that the custom hook can be called in the configuration without causing errors:\n",
    "    ```python\n",
    "    from .my_hook import MyHook\n",
    "    ``` \n",
    "\n",
    "    * Next, append '**MyHook**' to the __ all __ list. This makes the custom hook available for import when the module is loaded.\n",
    "    * The updated file should look like this:\n",
    "    ```python\n",
    "        # Copyright (c) OpenMMLab. All rights reserved.\n",
    "    from .checkloss_hook import CheckInvalidLossHook\n",
    "    from .mean_teacher_hook import MeanTeacherHook\n",
    "    from .memory_profiler_hook import MemoryProfilerHook\n",
    "    from .num_class_check_hook import NumClassCheckHook\n",
    "    from .pipeline_switch_hook import PipelineSwitchHook\n",
    "    from .set_epoch_info_hook import SetEpochInfoHook\n",
    "    from .sync_norm_hook import SyncNormHook\n",
    "    from .utils import trigger_visualization_hook\n",
    "    from .visualization_hook import (DetVisualizationHook,\n",
    "                                    GroundingVisualizationHook,\n",
    "                                    TrackVisualizationHook)\n",
    "    from .yolox_mode_switch_hook import YOLOXModeSwitchHook\n",
    "    from .my_hook import MyHook\n",
    "\n",
    "    __all__ = [\n",
    "        'YOLOXModeSwitchHook', 'SyncNormHook', 'CheckInvalidLossHook',\n",
    "        'SetEpochInfoHook', 'MemoryProfilerHook', 'DetVisualizationHook',\n",
    "        'NumClassCheckHook', 'MeanTeacherHook', 'trigger_visualization_hook',\n",
    "        'PipelineSwitchHook', 'TrackVisualizationHook',\n",
    "        'GroundingVisualizationHook', 'MyHook'\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "\n",
    "    * Now, the ***Custom Hook*** is fully integrated and ready to be used in your configuration without any issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Preparation of training dataset\n",
    "## 4.1 Introduction\n",
    "###  Formats for training datasets\n",
    "\n",
    "Model training needs training datasets, which are pairs of image data and label data. The datasets should be very accurate. Human visual interpretation of sampled image data is often employied for labeling image data.\n",
    "\n",
    "![Example of training data](https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/6349238b2269c6b312ce24ca_image10.webp)\n",
    "\n",
    "Source: [Labeling with LabelMe: Step-by-step Guide](https://www.v7labs.com/blog/labelme-guide)\n",
    "\n",
    "### Steps to prepare training datasets \n",
    "\n",
    "Here are steps to prepare training datasets with satellite images. You can skip this step since training dataset for this hands-on is already provided.\n",
    "\n",
    "1.\tLocating Harbor Scene:\n",
    "  *  Search for the harbor location in Google Earth with the extent\n",
    "  *  Set the eye altitude at 900m and cover the harbor locatio.\n",
    "  *  Export the scene in, including the harbor, as an image file. The guide for exporting the scene from Google Earth is well documented [here](https://glodal.sharepoint.com/:w:/s/GLODAL/EdXDpdTBwohKrJKTMi2QVEMBM2QvPrLwyH8eI6mcDM2owg?e=AqXyeG).\n",
    "\n",
    "2.\tAnnotating Cranes\n",
    "  *  Open the exported image in the ‘labelme’ tool\n",
    "  *  Use the ‘Create Polygons’ feature to draw the borders around target objects, such as cranes, in the image\n",
    "  *  Once the crane annotation is complete, label the polygon as target names, such as “crane”\n",
    "  *  Save the annotation, which will be stored in JSON format in the same folder as the image.\n",
    "\n",
    "3.\tConverting Annotations to COCO Format\n",
    "  *  Use the labelme2coco package to convert the JSON annotations from labelme to the COCO format\n",
    "  *  Apply the convert method provided by the labelme2coco to perform the conversion.\n",
    "\t\n",
    "This process ensures that the annotations are correctly labeled and converted to a standard coco data format for further analysis in object detection tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the demonstration dataset, call 'wget'. The dataset is already processed and ready to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O demo_dataset.zip http://owncloud-http/owncloud/index.php/s/S38aljMHL47rax2/download #Dataset class instances train:21, test:9   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset, it needs to be unzipped to fully use it for model training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(\"./demo_dataset.zip\", \"./demo_dataset\", \"zip\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Data split and conversion are required only if the dataset has not yet been processed (split) and converted into COCO format. In this demo, the dataset has already been processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data Split (optional)\n",
    "\n",
    "Training datasets should be split into two groups, training set and validation set. \"Training\" is dataset for iterative process of model training whereas \"testing\" is for evaluating trained models at every iteration. Strict model training sets three groups, training set, validation set, and test set. [See this article for a comprihensive explanation about splitting training datasets](https://mlu-explain.github.io/train-test-validation/).\n",
    "\n",
    "**NOTE**: This hands-on uses \"test\" though those are validation sets. The validation and test sets are often mixed in practice.\n",
    "\n",
    "The dataset is split by random sampling. The paths of all image files are collected and shuffled to ensure randomness. The dataset is then divided into 70% for the training set and 30% for the validation set, ensuring that the training and validation sets are randomly selected and non-overlapping. Once the images are annotated using LabelMe, the data folder will contain .tif image files along with .json files consisting of annotations. The script takes all the images from the ./data folder, splits the data into a 70:30 ratio between train and test sets, and places these sets inside the data_converted_to_coco folder, maintaining the original pairing between image files (.tif) and their corresponding annotation files (.json).\n",
    "\n",
    "The cell below is to process splitting, followed by converting them to [COCO format](https://cocodataset.org/#home), which is a popular format to train models for image recognition. This is optional because the downloaded dataset files have already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "orig_path = \"./data\"\n",
    "to_path = \"./data_converted_to_coco\"\n",
    "os.makedirs(os.path.join(to_path, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(to_path, \"test\"), exist_ok=True)\n",
    "\n",
    "path_ = glob.glob(os.path.join(orig_path, \"*.tif\"))\n",
    "\n",
    "random.shuffle(path_)\n",
    "split_index = math.ceil(len(path_) * 0.7)\n",
    "\n",
    "list_A = path_[:split_index]\n",
    "list_B = path_[split_index:]\n",
    "\n",
    "for x in list_A:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"train\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"train\", os.path.basename(json_path)))\n",
    "\n",
    "for x in list_B:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"test\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"test\", os.path.basename(json_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the splitted data to COCO format using labelme2coco as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labelme2coco\n",
    "\n",
    "labelme2coco.convert('./data_converted_to_coco/train','./data_converted_to_coco/train.json/')\n",
    "labelme2coco.convert('./data_converted_to_coco/test','./data_converted_to_coco/test.json/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Experiments\n",
    "\n",
    "To ensure that all required dependencies are correctly installed and configured before proceeding with further tasks, such as model training or fine-tuning, let's run the following command. This script collects and displays the versions of important packages to verify that the environment is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdet\n",
    "import os\n",
    "\n",
    "\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Configure model training\n",
    "\n",
    "Selecting a backbone is practically crucial for model training, as it determines complexity and computing cost. [The recent versions of MMDetection provides a variety of backbones for users to experiment with](https://mmdetection.readthedocs.io/en/latest/model_zoo.html). Loading and adjusting the configuration is necessary to meet specific objectives.\n",
    "\n",
    "In this hands-on, we use Faster R-CNN with ResNet101. This is rather simple than the recent models, so good for initial practices.\n",
    "\n",
    "The settings to configure include:\n",
    "\n",
    "* Path to the dataset\n",
    "* Model configurations \n",
    "* Training hyperparameters, such as learning rate, batch size, number of epochs, etc.\n",
    "* Data augmentation\n",
    "* Optimizer configurations\n",
    "\n",
    "When the config file is edited, use the mmdetection/tools/train.py script provided by MMDetection to start training. This script takes the configuration file as input and handles the training process. During training, the script saves checkpoints at specified intervals, allowing for resuming training or evaluating the model at different stages. The training process can be monitored using command-line outputs and log files. MMDetection also supports TensorBoard for visualizing training metrics like loss and accuracy. Hyperparameters can be adjusted if necessary based on the observed training behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clone the mmdetection repository to your local machine, run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~  # Optional: Ensures you're in the home directory\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will download all the files from the mmdetection repository to a directory named mmdetection on your local machine, enabling you to start using and modifying the code right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the configuration correctly within mmdetection/configs, you need to import the configuration module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading the configuration and outputting its structure is necessary to understand the default configuration of the selected backbone and identify the variables that need to be changed.\n",
    "\n",
    "    To find a config path, e.g., Faster R-CNN:\n",
    "When the config file is edited, use the mmdetection/tools/train.py script provided by MMDetection to start training. This script takes the configuration file as input and handles the training process. During training, the script saves checkpoints at specified intervals, allowing for resuming training or evaluating the model at different stages. The training process can be monitored using command-line outputs and log files. MMDetection also supports TensorBoard for visualizing training metrics like loss and accuracy. Hyperparameters can be adjusted if necessary based on the observed training behavior.\n",
    "\n",
    "\n",
    "To load the configuration correctly within mmdetection/configs, you need to import the configuration module like below. The file names indicates models and backboens. You may choose a pre-defined model architecture from the MMDetection model zoo, such as RPN, Faster R-CNN, Mask R-CNN, RetinaNet, etc. MMDetection provides config files for various pre-trained models. You can edit the config file to match the dataset and training settings though default configuration usually works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "\n",
    "cfg = Config.fromfile('./mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Setup model configurations\n",
    "\n",
    "After loading and outputting the configuration, there are three main parts that are important to check, update, and double-check:\n",
    "1. Path to data files and config files.\n",
    "2. Model training parameters such as batch size, number of classes, type of model, and losses. These are essential for fine-tuning but you should check before training.\n",
    "3. Train, validation, and test dataloaders, including the pipeline responsible for the datasets used during training, evaluation, and testing.\n",
    "\n",
    "Set the important keys or data in the config dictionary, such as dataset root, output model, and other relevant parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_root = './demo_dataset/'  # data_root means the path of the dataset that will be used.\n",
    "cfg.dataset_type = 'CocoDataset'   # type of the dataset, indicating its structure. Mostly it's CocoDataset.\n",
    "\n",
    "# Set auto scaling of learning rate parameters:\n",
    "#   - base_batch_size: Batch size used as a base for scaling.\n",
    "#   - enable: Flag to enable auto scaling of learning rate.\n",
    "cfg.auto_scale_lr = dict(base_batch_size=16, enable=True)  # auto_scale will auto set its batch size based on the selected batch size of the dataloaders.\n",
    "cfg.backend_args = None  # Configure backend arguments.\n",
    "\n",
    "cfg.work_dir = './model'  # Output path of the model.\n",
    "\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the previous output is now updated and to reduce the risk of errors during training, double-check the keys that were updated. Here’s how you can verify the updates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data root: {cfg.data_root}\")\n",
    "print(f\"Dataset type: {cfg.dataset_type}\")\n",
    "print(f\"Auto scale LR: {cfg.auto_scale_lr}\")\n",
    "print(f\"Backend args: {cfg.backend_args}\")\n",
    "print(f\"Work directory: {cfg.work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Model Setup\n",
    "Choose a pre-defined model architecture from the MMDetection model zoo, such as RPN, Faster R-CNN, Mask R-CNN, RetinaNet, etc. MMDetection provides config files for various pre-trained models. Edit the config file to match the dataset and training settings.\n",
    "\n",
    "Default configuration mostly works well, but you should pay attention to some key parameters. For example, the number of classesatch the class number of the dataset used. While updating the loss and other keys is not always necessary, basic knowledge of the model structure is required if changes are needed. For this example, Faster R-CNN is used, but there are diverse backbones that can be employed.\n",
    "\n",
    "First, check the model structure to find the keys that should be updated and verify the default values to ensure the model meets your objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.model)  # Print the model descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are examples of how to update some key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update backbone parameters\n",
    "cfg.model['backbone']['depth'] = 101 # change FasterRCNN backbone depth\n",
    "cfg.model['backbone']['init_cfg']['checkpoint'] = 'torchvision://resnet101' # path/to/custom/pretrained.pth; Specify custom pretrained weights\n",
    "cfg.model['backbone']['init_cfg']['type'] = 'Pretrained' # specifies how the model should be initialized or where it should load its initial weights from\n",
    "cfg.model['backbone']['norm_cfg']['requires_grad'] = False  # Disable gradient updates for normalization layer\n",
    "# Update neck parameters\n",
    "cfg.model['neck']['in_channels'] = [256, 512, 1024, 2048]  # Add an additional stage with 4096 input channels\n",
    "cfg.model['neck']['out_channels'] = 256  # Increase output channels to 512\n",
    "\n",
    "# Update ROI Head parameters\n",
    "cfg.model['roi_head']['bbox_head']['loss_bbox']['loss_weight'] = 1.0  # Increase weight for bounding box regression loss\n",
    "cfg.model['roi_head']['bbox_head']['num_classes'] = 2  # Change number of classes to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the number of classes is updated. The model structure is now ready to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Data Loading\n",
    "Properly loading and configuring the dataset ensures that the model receives the data in the right format and structure, allowing for effective learning and evaluation during the training, validation, and testing phases. Proper augmentation, batch size, and workers are necessary to ensure the process is error-free and efficient, preventing issues like insufficient memory or incorrect model predictions due to poor augmentation combinations.\n",
    "\n",
    " 1. **Training Dataloader Setup**: Proper augmentation, batch size, and workers are necessary to ensure the process is error-free and efficient, preventing issues like insufficient memory or incorrect model predictions due to poor augmentation combinations.\n",
    " \n",
    "    First, print the current structure of the training dataloader and its training configuration to check defaults and settings for epochs and validation intervals.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.train_dataloader)  # Print current training dataloader structure\n",
    "print(cfg.train_cfg)  # Print training configuration (epochs, validation interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train pipeline** : The augmentations the dataset will undergo before training. Proper augmentation results in better model performance. Proper augmentation results in better model performance. [See also the official document for details](https://mmdetection.readthedocs.io/en/latest/advanced_guides/transforms.html?highlight=train_pipeline#design-of-data-transforms-pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_pipeline = [\n",
    "    dict(backend_args=None, type='LoadImageFromFile'),  # Load image from file\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    #dict(type='CachedMosaic', img_scale=(1024, 1024), pad_val=114.0),  # Cached Mosaic augmentation to reduce memory consumption\n",
    "    dict(type='Resize', keep_ratio=True, scale=(1333, 800)),  # Resize images to the target size\n",
    "    dict(type='RandomFlip', prob=0.5),  # Apply random horizontal flip with 50% probability\n",
    "    #dict(\n",
    "    #    type='CachedMixUp',\n",
    "    #    img_scale=(1024, 1024),  # Cached MixUp augmentation\n",
    "    #    ratio_range=(1.0, 1.0),\n",
    "    #    max_cached_images=20,\n",
    "    #    pad_val=(114, 114, 114)),\n",
    "    dict(type='PackDetInputs'),  # Pack inputs for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the **Train Dataloader** setup, updating batch size for memory efficiency, setting the dataset's training JSON file, and configuring the pipeline to avoid redundant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update batch size for the training dataloader\n",
    "cfg.train_dataloader['batch_size'] = 1  # Set batch size to 2\n",
    "cfg.train_dataloader['num_workers'] = 1\n",
    "\n",
    "# Set dataset type\n",
    "cfg.train_dataloader['dataset']['type'] = cfg.dataset_type\n",
    "\n",
    "# Configure training epochs and validation interval\n",
    "cfg.train_cfg['max_epochs'] = 100  # Maximum number of training epochs\n",
    "cfg.train_cfg['val_interval'] = 100  # Interval (in epochs) for validation\n",
    "\n",
    "# Set the annotation file for training dataset\n",
    "cfg.train_dataloader['dataset']['ann_file'] = 'train.json'  # Path to training annotations\n",
    "\n",
    "# Initialize metainfo dictionary if not present\n",
    "cfg.train_dataloader['dataset']['metainfo'] = {}  \n",
    "cfg.train_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "# Set data root directory and image prefix\n",
    "cfg.train_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.train_dataloader['dataset']['data_prefix']['img'] = \"train/images\"  # Path to training images\n",
    "\n",
    "# Assign the train pipeline\n",
    "cfg.train_dataloader['dataset']['pipeline'] = cfg.train_pipeline  # Apply training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. **Validtion and Test Dataloader Setup**: Validation and Test Dataloader Setup: Both should be configured similarly, as the validation dataloader is used during the model's validation phase, and the test dataloader is used during model evaluation, such as running the test.py script and for generating a confusion matrix.\n",
    "\n",
    "    The reason for minimal augmentation is to match real-world images and scenarios.\n",
    "      * Validation Dataloader: Print its default structure before updating.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.val_dataloader)  # Print current validation dataloader structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the training dataloader, the validation dataloader uses the default pipeline with updated dimensions to match the dataset used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['val_pipeline'] = cfg.test_pipeline\n",
    "cfg.val_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the training dataloader structure, update the JSON and classes, then print to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading validation data during training\n",
    "cfg.val_dataloader['batch_size'] = 1  # Update the batch size for the validation dataloader\n",
    "cfg.val_dataloader['num_workers'] = 1\n",
    "cfg.val_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.val_dataloader['dataset']['ann_file'] = 'test.json'  # Path to validation annotations\n",
    "\n",
    "cfg.val_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.val_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.val_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.val_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to validation images\n",
    "\n",
    "cfg.val_dataloader['dataset']['pipeline'] = cfg.val_pipeline  # Apply validation pipeline\n",
    "\n",
    "#For the val evaluator\n",
    "cfg.val_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")\n",
    "\n",
    "print(cfg.val_dataloader)  # Print updated validation dataloader structure\n",
    "print(cfg.val_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The JSON file is set to test.json instead of val.json because the demo dataset does not contain validation data. However, this can be changed if validation data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test Dataloader**: The test dataloader is used during model evaluation, such as running the test.py script and generating a confusion matrix.\n",
    "\n",
    "    First, set up the test pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the test dataloader with necessary updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for testing dataset for confusion matrix and model evaluation after training\n",
    "cfg.test_dataloader['batch_size'] = 1  # Update the batch size for the test dataloader\n",
    "cfg.test_dataloader['num_workers'] = 1\n",
    "cfg.test_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.test_dataloader['dataset']['ann_file'] = 'test.json'  # Path to test annotations\n",
    "\n",
    "cfg.test_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.test_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.test_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.test_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to test images\n",
    "\n",
    "cfg.test_dataloader['dataset']['pipeline'] = cfg.test_pipeline  # Apply test pipeline\n",
    "\n",
    "cfg.test_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")  # Set annotation file for the evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 Setup custom hook\n",
    "#### Custom Hook\n",
    "The configuration is already set; now it's time to add the custom hook created earlier.\n",
    "* Define custom_hooks with MyHook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['custom_hooks'] = [dict(type='MyHook')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the configuration is done, the next step is to save it. MMDetection (3.3.0) config library has a dump feature to save it. This will save to the model output path that was set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.work_dir, exist_ok = True)\n",
    "cfg.dump(os.path.join(cfg.work_dir, \"config.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run the model training\n",
    "\n",
    "\n",
    "To start model training, run the following script:\n",
    "```\n",
    "python mmdetection/tools/train.py {config path}\n",
    "```  \n",
    "This command will initiate the model training process according to the settings specified in the configuration file, continuing until the specified number of epochs is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmdetection/tools/train.py \"./model/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Evaluating detection performance\n",
    "\n",
    "Mean Average Precision (mAP) is a metric used to measure the performance of a model for tasks such as object detection tasks and information retrieval. It is is a widely used for evaluating the performance of object detection models. It summarizes the precision-recall curve and provides a single number representing the overall performance of the model. [You may find comprehensive details with this article.](https://towardsdatascience.com/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models-1ea4f67a9dbd)\n",
    "\n",
    "\n",
    "* Precision: The ratio of true positive detections to the total number of detections (true positives + false positives).\n",
    "* Recall: The ratio of true positive detections to the total number of ground truth instances (true positives + false negatives).\n",
    "* Average Precision (AP): The area under the precision-recall curve for a single class. It is computed by taking the average of precision values at different recall levels.\n",
    "* Mean Average Precision (mAP): The mean of APs across all classes. It gives an overall performance measure of the detection model across different object categories.\n",
    "\n",
    "In the context of object detection, a model's performance is often reported using mAP at different Intersection over Union (IoU) thresholds (e.g., `mAP@0.5`, `mAP@0.75`, `mAP@[0.5:0.95]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#change the path, folder log differ everytime.\n",
    "with open('./model/20240724_121658/vis_data/20240724_121658.json', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "current_epoch = None\n",
    "line_number = 0\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    entry = json.loads(line.strip())\n",
    "    if 'epoch' in entry:\n",
    "        train_losses.append((entry['epoch'], entry['iter'], entry['loss_bbox']))\n",
    "        current_epoch = entry['epoch']\n",
    "    elif 'loss' in entry:\n",
    "        val_losses.append((current_epoch, entry['iter'], entry['loss_bbox']))\n",
    "\n",
    "train_epochs, train_iters, train_loss_values = zip(*train_losses)\n",
    "if val_losses:\n",
    "    val_epochs, val_iters, val_loss_values = zip(*val_losses)\n",
    "    plt.plot(val_epochs, val_loss_values, label='Validation Loss')\n",
    "\n",
    "plt.plot(train_epochs, train_loss_values, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (loss_bbox)')\n",
    "plt.title('Detection Learning Curve for Crane')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Evaluating classification performance\n",
    "Confusion Matrix is a valuable tool for evaluating the performance of a classification model, including object detection models. It provides a detailed breakdown of the model's predictions, which helps in understanding the strengths and weaknesses of the model. The matrix consists of four categories as below. [You may find comprehensive details with this article.](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "* **True Positives (TP)**: The model correctly identifies an object that is present.\n",
    "* **False Positives (FP)**: The model incorrectly identifies an object that is not present (**false alarm**).\n",
    "* **True Negatives (TN)**: The model correctly identifies the absence of an object (**background)**.\n",
    "* **False Negatives (FN)**: The model fails to identify an object that is present.\n",
    "\n",
    "In object detection, the Confusion Matrix may include a background class even if it’s not explicitly defined. This is because the model needs to distinguish between objects and non-objects (**background)**. The matrix helps assess how well the model detects objects and handles background areas.\n",
    "\n",
    "Before creating a confusion matrix in MMDetection, a pickle file must be generated using **test.py**, which will test the model and use the dataloader (**test_dataloader**) to create the pickle. This pickle file is then used for generating the confusion matrix. Running the following command will generate a .pkl file necessary for the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python mmdetection/tools/test.py {config_path} {model_path} --out {pickle_output_path}\n",
    "\n",
    "#example code\n",
    "!python ./mmdetection/tools/test.py \"./model/config.py\" \"./model/epoch_100.pth\" --out \"./result.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MMDetection’s confusion matrix has not been updated to handle custom outputs, a custom script is used for model evaluation. This script will produce a plot image that provides a detailed assessment of the model’s performance. The plot will include:\n",
    "\n",
    "* Classes and background: A visual representation of detected classes and background.\n",
    "* Overall Count of ground truth per Class: The total number of true instances for each class.\n",
    "* Count of incorrect background detections: Instances where the model incorrectly identified the background.\n",
    "\n",
    "This approach ensures an accurate evaluation of the model by highlighting both correct detections and areas of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.fileio import load\n",
    "from mmdet.utils import replace_cfg_vals\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmdet.registry import DATASETS\n",
    "from mmdet.evaluation import bbox_overlaps\n",
    "from mmengine.config import Config\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_confusionmatrix(config_path, pkl_path, score_thr = 0.3, tp_iou_thr = 0.5):\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    cfg = replace_cfg_vals(cfg)\n",
    "    init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "    results = load(pkl_path)\n",
    "    dataset = DATASETS.build(cfg.test_dataloader.dataset)\n",
    "\n",
    "    assert len(results) == len(dataset), \"Please check the dataset\"\n",
    "\n",
    "    num_classes = len(dataset.metainfo['classes'])\n",
    "    confusion_matrix = np.zeros(shape=[num_classes + 1, num_classes + 1])\n",
    "    for idx, per_img_res in enumerate(results):\n",
    "        res_bboxes = per_img_res['pred_instances']\n",
    "        gts = dataset.get_data_info(idx)['instances']\n",
    "    \n",
    "        true_positives = np.zeros(len(gts))\n",
    "        gt_bboxes = []\n",
    "        gt_labels = []\n",
    "        for gt in gts:\n",
    "            gt_bboxes.append(gt['bbox'])\n",
    "            gt_labels.append(gt['bbox_label'])\n",
    "        gt_bboxes = np.array(gt_bboxes)\n",
    "        gt_labels = np.array(gt_labels)\n",
    "    \n",
    "        unique_label = np.unique(res_bboxes['labels'].numpy())\n",
    "    \n",
    "        for det_label in unique_label:\n",
    "            mask = (res_bboxes['labels'] == det_label)\n",
    "            det_bboxes = res_bboxes['bboxes'][mask].numpy()\n",
    "            det_scores = res_bboxes['scores'][mask].numpy()\n",
    "        \n",
    "            ious = bbox_overlaps(det_bboxes[:, :4], gt_bboxes)\n",
    "            for i, score in enumerate(det_scores):\n",
    "                det_match = 0\n",
    "                if score >= score_thr:\n",
    "                    for j, gt_label in enumerate(gt_labels):\n",
    "                        if ious[i, j] >= tp_iou_thr:\n",
    "                            det_match += 1\n",
    "                            if gt_label == det_label:\n",
    "                                true_positives[j] += 1  # TP\n",
    "                            confusion_matrix[gt_label, det_label] += 1\n",
    "                    if det_match == 0:  # BG FP\n",
    "                        confusion_matrix[-1, det_label] += 1\n",
    "        for num_tp, gt_label in zip(true_positives, gt_labels):\n",
    "            if num_tp == 0:  # FN\n",
    "                confusion_matrix[gt_label, -1] += 1\n",
    "\n",
    "    class_labels = ['Gantry Crane', 'Standby Gantry Crane', 'Background']\n",
    "    additional_info = confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(np.arange(confusion_matrix.shape[1]))\n",
    "    ax.set_yticks(np.arange(confusion_matrix.shape[0]))\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            ax.text(j, i, int(confusion_matrix[i, j]), ha='center', va='center', color='black')\n",
    "    \n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    \n",
    "    for idx, info in enumerate(additional_info):\n",
    "        plt.text(0.5, -0.2 - idx * 0.1, f\"{class_labels[idx]}: {info}\", ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "config_path = \"./model/config.py\" #configuration path\n",
    "pkl__path = \"./result.pkl\" #pickel outpath\n",
    "\n",
    "generate_confusionmatrix(config_path, pkl__path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the confusion matrix reveals errors like false positives and false negatives, guiding improvements in the model’s performance, such as tuning parameters, refining training data, or adjusting detection techniques.\n",
    "\n",
    "\n",
    "### 5.3.3 Model Inference\n",
    "For inference visuazalition, MMDetection provides a script for visualizing inference result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "path_model = \"./model/epoch_100.pth\"\n",
    "path_config = \"./model/config.py\"\n",
    "\n",
    "detection_model = DetInferencer(model=path_config, weights=path_model, show_progress=True)\n",
    "\n",
    "detection_model('./demo_dataset/test/images', out_dir='./inference_result', no_save_pred=False, pred_score_thr = 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: An error might occur during processing, as MMDetection version 3 has not yet issued a fix. Manual adjustment to the script may be necessary.\n",
    "\n",
    "For a quick fix in MMDetection version 3, a modification is needed in the runtime_info_hook.py file located in the mmengine/hooks/ folder. Specifically, on line 132, within the after_test_iter function, the following changes should be made:\n",
    "\n",
    "1. Open the runtime_info_hook.py file.\n",
    "2. Locate the after_test_iter function around line 132.\n",
    "3. Add a try and except block to handle the error related to custom validation loss, which occurs when generating the pickle file. The updated function might look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def after_test_iter(self,\n",
    "                    runner,\n",
    "                    batch_idx: int,\n",
    "                    data_batch: DATA_BATCH = None,\n",
    "                    outputs: Optional[dict] = None) -> None:\n",
    "\n",
    "    if outputs is not None:\n",
    "        try:\n",
    "            for key, value in outputs.items():\n",
    "                runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adjustment will help bypass the error caused by custom validation loss when generating the pickle file, as this issue arises from using the script specifically for generating the pickle rather than for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Improve model training\n",
    "The models can be improved by performing combination of data augmentation, model configurations, hyperparameter tuning and some other techniques.\n",
    "\n",
    "## 6.1 Data Augmentation:\n",
    "\n",
    "By performing various augmentations to the training datasets, it can make the model robust and improve generalization. In mmdetection, ‘pipeline’ contains preprocessors of dataset such as augmentation. Some techniques are: Resize, RandomFlip, Normalize, Pad, RandomCrop, ColorTransform, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the following parameter to cfg file to use mentioned data augmentation.\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='RandomCrop', crop_size=(800, 800)),\n",
    "    dict(type='ColorTransform', prob=0.5, level=1),\n",
    "    dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n",
    "]\n",
    "\n",
    "# The choice of selecting augmentation depends upon dataset characteristics, task requirements and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Exploring alternative models and datasets for model training\n",
    "Pre-trained detectors from the COCO dataset serve as effective models for initializing on another dataset. Using models available in the Model Zoo can significantly enhance performance. To fine-tune a model for a new dataset, follow these steps:\n",
    "\n",
    "1. **Inherit default configs:** mmdetection supports inheriting configurations from existing setups. Start by inheriting base configurations for model architecture (models/), dataset specifics (datasets/), and runtime settings (default_runtime.py) from mmdetection’s configs directory.\n",
    "2. **Modify configurations:** Adjust settings such as model backbone, ROI heads, and dataset paths to suit the characteristics of the new dataset. Modify parameters like num_classes in the ROI head to match the number of classes in the new dataset.\n",
    "3. **Load pre-trained weights:** Initialize the model with weights pretrained on a large-scale dataset (e.g., COCO). This step initializes the model with beneficial learned features for object detection tasks.\n",
    "4. **Fine-tuning:** Fine-tune the initialized model using the new dataset. Optimize hyperparameters such as learning rate, optimizer type, and batch size to improve performance. Considerations include adjusting learning rates and epochs based on the dataset’s scale and complexity.\n",
    "\n",
    "To use a pre-trained model, specify the path to the pretrained checkpoint in load_from. Ensure the model weights are downloaded before training to minimize download time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.load_from = 'path_to_pretrained_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model configurations:** To enhance the object detection capabilities, we can explore alternative models such as ResNet. ResNet101 is known for its efficiency and accuracy in real-time object detection tasks. Here’s how it can be configured and train a ResNet101 model using your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be adjusted with following component within cfg.model\n",
    "model = dict(\n",
    "    backbone=dict(  # Define the backbone of the model (usually a pre-trained network for feature extraction)\n",
    "        depth=101,  # Specifies the depth of the ResNet, in this case, ResNet-101 (101 layers)\n",
    "        frozen_stages=1,  # The first stage of the network is frozen (not updated during training)\n",
    "        init_cfg=dict(checkpoint='torchvision://resnet101', type='Pretrained'),  # Initialization configuration: uses a pre-trained ResNet-101 model from Torchvision\n",
    "        norm_cfg=dict(requires_grad=False, type='BN'),  # Normalization configuration: batch normalization (BN) layers with fixed parameters (no gradient update)\n",
    "        norm_eval=True,  # During evaluation, batch normalization layers use running statistics rather than batch statistics\n",
    "        num_stages=4,  # The number of stages in the ResNet architecture (ResNet typically has 4 stages)\n",
    "        out_indices=(\n",
    "            0,\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "        ),  # Output features from all 4 stages of the network (0 to 3)\n",
    "        style='pytorch',  # Indicates that the ResNet architecture follows the original PyTorch implementation\n",
    "        type='ResNet'),  # Specifies that the backbone type is ResNet\n",
    "# The overall architecture is a Feature Pyramid Network (Resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load the entire configuration provided by MMDetection. To load the configuration, use:\n",
    "\n",
    "```\n",
    "cfg = Config.fromfile('mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)\n",
    "```\n",
    "\n",
    "Then, follow the guide in section 5. The flow remains the same; the differences lie in augmentation, batch size, or custom hooks like RTMDet. A prior understanding of what has been done is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Hyperparameter tuning:\n",
    "\n",
    " It involves adjusting the settings of optimizers. These settings, known as hyperparameters, are not learned from the training data but set prior to the training process. Effective tuning of hyperparameters such as learning rate, batch size, optimizer type, and the number of epochs can significantly impact the model’s accuracy and convergence speed. In mmdetection, hyperparameters are configured in the model’s configuration file and can be fine-tuned based on the specific dataset and task requirements to achieve better performance. We have a parameter scheduler, which dynamically adjusts learning rates and other hyperparameter during training to enhance model convergence and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Similar to section 5, many of the cells in this section will be identical, with the primary differences being in the values and additional dictionaries used in the augmentation pipeline. The changes primarily focus on adjusting values and the train_dataloader.\n",
    "\n",
    "    First, load the configuration as in section 5. Notice that most of the cells will be identical to those in section 5, but with some modifications to suit the current setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "import os\n",
    "\n",
    "cfg = Config.fromfile('./mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set the important keys or data in the configuration dictionary, such as the dataset root, output model, and other relevant parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_root = './demo_dataset/'  # data_root means the path of the dataset that will be used.\n",
    "cfg.dataset_type = 'CocoDataset'   # type of the dataset, indicating its structure. Mostly it's CocoDataset.\n",
    "\n",
    "cfg.auto_scale_lr = dict(base_batch_size=16, enable=True)  # auto_scale will auto set its batch size based on the selected batch size of the dataloaders.\n",
    "cfg.backend_args = None  # Configure backend arguments.\n",
    "\n",
    "cfg.work_dir = './model_finetuned'  # Output path of the model.\n",
    "\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model setup is identical to the previous configuration, but there are some values that can be adjusted. These adjustments require a deep understanding of the model, so for now, we'll stick to the basics that still contribute to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update backbone parameters\n",
    "cfg.model['backbone']['depth'] = 101 # change FasterRCNN backbone depth\n",
    "cfg.model['backbone']['init_cfg']['checkpoint'] = 'torchvision://resnet101' # path/to/custom/pretrained.pth; Specify custom pretrained weights\n",
    "cfg.model['backbone']['init_cfg']['type'] = 'Pretrained' # specifies how the model should be initialized or where it should load its initial weights from\n",
    "cfg.model['backbone']['norm_cfg']['requires_grad'] = False  # Disable gradient updates for normalization layer\n",
    "# Update neck parameters\n",
    "cfg.model['neck']['in_channels'] = [256, 512, 1024, 2048]  # Add an additional stage with 4096 input channels\n",
    "cfg.model['neck']['out_channels'] = 256  # Increase output channels to 512\n",
    "\n",
    "# Update ROI Head parameters\n",
    "cfg.model['roi_head']['bbox_head']['loss_bbox']['loss_weight'] = 1.0  # Increase weight for bounding box regression loss\n",
    "cfg.model['roi_head']['bbox_head']['num_classes'] = 2  # Change number of classes to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_pipeline = [\n",
    "    dict(backend_args=None, type='LoadImageFromFile'),  # Load image from file\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(type='Resize', keep_ratio=True, scale=(1024, 1024)),  # Resize images to the target size\n",
    "    dict(crop_size=(\n",
    "                1024,\n",
    "                1024,\n",
    "            ), type='RandomCrop'),\n",
    "    dict(type='YOLOXHSVRandomAug'),\n",
    "    dict(\n",
    "        direction=[\n",
    "            'horizontal',\n",
    "            'vertical',\n",
    "            'diagonal',\n",
    "        ],\n",
    "        prob=0.75,\n",
    "        type='RandomFlip'), # Apply random horizontal flip with 75% probability\n",
    "    dict(level=10, prob=1.0, type='Color'),\n",
    "    dict(level=10, prob=1.0, type='AutoContrast'),\n",
    "    dict(level=6, prob=1.0, type='Brightness'),\n",
    "    dict(level=4, prob=1.0, type='Sharpness'),\n",
    "    dict(\n",
    "                pad_val=dict(img=(\n",
    "                    114,\n",
    "                    114,\n",
    "                    114,\n",
    "                )),\n",
    "                size=(\n",
    "                    1024,\n",
    "                    1024,\n",
    "                ),\n",
    "                type='Pad'),\n",
    "    dict(type='PackDetInputs'),  # Pack inputs for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update batch size for the training dataloader\n",
    "cfg.train_dataloader['batch_size'] = 4  # Set batch size to 2\n",
    "cfg.train_dataloader['num_workers'] = 1\n",
    "\n",
    "# Set dataset type\n",
    "cfg.train_dataloader['dataset']['type'] = cfg.dataset_type\n",
    "\n",
    "# Configure training epochs and validation interval\n",
    "cfg.train_cfg['max_epochs'] = 100  # Maximum number of training epochs\n",
    "cfg.train_cfg['val_interval'] = 100  # Interval (in epochs) for validation\n",
    "\n",
    "# Set the annotation file for training dataset\n",
    "cfg.train_dataloader['dataset']['ann_file'] = 'train.json'  # Path to training annotations\n",
    "\n",
    "# Initialize metainfo dictionary if not present\n",
    "cfg.train_dataloader['dataset']['metainfo'] = {}  \n",
    "cfg.train_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "# Set data root directory and image prefix\n",
    "cfg.train_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.train_dataloader['dataset']['data_prefix']['img'] = \"train/images\"  # Path to training images\n",
    "\n",
    "# Assign the train pipeline\n",
    "cfg.train_dataloader['dataset']['pipeline'] = cfg.train_pipeline  # Apply training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['val_pipeline'] = cfg.test_pipeline\n",
    "cfg.val_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1024, 1024), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading validation data during training\n",
    "cfg.val_dataloader['batch_size'] = 2  # Update the batch size for the validation dataloader\n",
    "cfg.val_dataloader['num_workers'] = 1\n",
    "cfg.val_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.val_dataloader['dataset']['ann_file'] = 'test.json'  # Path to validation annotations\n",
    "\n",
    "cfg.val_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.val_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.val_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.val_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to validation images\n",
    "\n",
    "cfg.val_dataloader['dataset']['pipeline'] = cfg.val_pipeline  # Apply validation pipeline\n",
    "\n",
    "#For the val evaluator\n",
    "cfg.val_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")\n",
    "\n",
    "print(cfg.val_dataloader)  # Print updated validation dataloader structure\n",
    "print(cfg.val_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Val Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1024, 1024), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for testing dataset for confusion matrix and model evaluation after training\n",
    "cfg.test_dataloader['batch_size'] = 2  # Update the batch size for the test dataloader\n",
    "cfg.test_dataloader['num_workers'] = 1\n",
    "cfg.test_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.test_dataloader['dataset']['ann_file'] = 'test.json'  # Path to test annotations\n",
    "\n",
    "cfg.test_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.test_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.test_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.test_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to test images\n",
    "\n",
    "cfg.test_dataloader['dataset']['pipeline'] = cfg.test_pipeline  # Apply test pipeline\n",
    "\n",
    "cfg.test_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")  # Set annotation file for the evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Set Custom Hook and the Output/Dump Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['custom_hooks'] = [dict(type='MyHook')]\n",
    "\n",
    "os.makedirs(cfg.work_dir, exist_ok = True)\n",
    "cfg.dump(os.path.join(cfg.work_dir, \"config.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's ready to be trained again, and you can compare the differences from the previous model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Run the tuned model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmdetection/tools/train.py \"./model_finetuned/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model training, run the code/cell in section 5.4.2 to evaluate classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
